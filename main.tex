\documentclass[11pt,oneside]{article}


%\usepackage[latin1]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{comment} % Added to define the comment environment
\usepackage{graphicx} % Required for \resizebox
\usepackage{lineno}
\usepackage{xcolor}
\usepackage{amsmath} 
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmaxmin}{arg\,max\,min}


\usepackage{booktabs}

\usepackage{longtable}
\usepackage{array}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem*{question*}{\protect\questionname}
\theoremstyle{plain}
%\newtheorem{example}{\protect\examplename}
\newtheorem{example}{Example}[section]
\theoremstyle{plain}
%\newtheorem{thm}{\protect\theoremname}
\newtheorem{thm}{Theorem}[section]
\theoremstyle{plain}
%\newtheorem{cor}{\protect\corollaryname}
\newtheorem{corollary}{Corollary}[section]
\theoremstyle{plain}
\newtheorem{assumption}[thm]{\protect\assumptionname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{definition}
\newtheorem{prop}{Proposition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}

\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\essinf}{ess\,inf}

\providecommand{\assumptionname}{Assumption}
\providecommand{\examplename}{Example}
\providecommand{\definitionname}{Definition}
\providecommand{\corollaryname}{Corollary}
\providecommand{\lemmaname}{Lemma}
\providecommand{\questionname}{Question}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\providecommand{\keywords}[1]{\textbf{\textit{Keywords:}} #1}


\newcommand{\dd}{\mathsf {d\kern -0.07em l}} %
\newcommand{\R}{{\rm I\!R}}
\def\bbe{{\mathbb{E}}}
\newcommand{\bgeqn}{\begin{eqnarray}}
\newcommand{\edeqn}{\end{eqnarray}}
\newcommand{\bgeq}{\begin{eqnarray*}}
\newcommand{\edeq}{\end{eqnarray*}}
\newcommand{\F}{{\cal F}}

\newcommand{\ulf}{\underline{f}}
\newcommand{\olf}{\overline{f}}
\newcommand{\ulh}{\underline{h}}
\newcommand{\olh}{\overline{h}}
\newcommand{\pfo}{\pf_\omega}

\newcommand{\calps}{{\cal P}({\cal S})}
\newcommand{\calpx}{{\cal P}({\cal X})}
\newcommand{\calppsi}{{\cal P}(\Psi)}


\renewcommand{\L}{{\cal L}}
\renewcommand{\Re}{\R}
\newcommand{\transpose}{\mathsf{T}}
\newcommand{\ind}[1]{\mathds{1}_{\{#1\}}}
\newcommand{\beq}[1]{\begin{linenomath}\begin{align} #1 \end{align}\end{linenomath}}
\newcommand{\beqq}[1]{\begin{linenomath}\begin{align*} #1 \end{align*}\end{linenomath}}
\renewcommand{\Re}{\R}
\newcommand{\Na}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}_{+}}
\newcommand{\ex}[1]{\mathds{E}\left[#1\right]}
\newcommand{\pr}[1]{\mathds{P}\left\{#1\right\}}
\renewcommand{\sp}{\texttt{span}}
\newcommand{\pas}{\;\;P-\text{a.s.}}
\newcommand{\mcs}{{\cal S}}
\newcommand{\mcx}{{\cal X}}
\renewcommand{\matrix}[2]{\left[\begin{array}{#1} #2 \end{array}\right]}
\newcommand{\pf}{\succeq}
\newcommand{\pfs}{\succeq^*}
\newcommand{\lsf}{l_s^f}
\newcommand{\convmix}[2]{\alpha #1 + (1-\alpha) #2}
\newcommand{\convexmix}[3]{#3 #1 + (1-#3) #2}
\newcommand{\smix}[2]{#1_s #2 }
\newcommand{\omix}[2]{#1_\omega #2 }
\newcommand{\psimix}[2]{#1_{\psi} #2}
\newcommand{\wh}[1]{{\color{blue} #1     }}
\newcommand{\bb}[1]{{\color{red} #1     }}
\newcommand{\tdf}{\tilde{f}}
\newcommand{\dee}{\mathrm{d}}
\newcommand{\indicator}{\mathds{1}}
\newcommand{\ulfo}{\ulf[\omega]}
\newcommand{\olfo}{\olf[\omega]}
\title{Preference Robust Ambiguity-Averse Optimization}
\author{Bingrui Bian and William B. Haskell}
\date{\today}
%November 2023}


\begin{document}

\maketitle


\begin{abstract}
In stochastic optimization, the decision maker (DM) must make a choice in the face of future uncertainty.
Proper formulation of this optimization problem requires specification of the DM's beliefs (about the likelihood of the underlying states of the world) and tastes (over fixed outcomes).
The decision theory literature has significantly investigated ambiguity in DM's beliefs (for fixed tastes).
Yet, in practice there is simultaneously ambiguity in both DM's beliefs and tastes.
We propose a choice model to account for this joint ambiguity in beliefs and tastes in the setting of Anscombe-Aumann acts.
We identify the behavioral axioms that characterize this model, and derive a representation for this class of preferences.
In addition, we show that our model is computationally tractable. We illustrate our model numerically in a social welfare problem, which depends on the preferences of a population of agents.
\end{abstract}

\keywords 
Distributionally robust optimization, preference robust optimization, ambiguity aversion, behavioral axioms, representation


\section{Introduction}

Stochastic optimization is a framework for decision-making in the face of uncertainty. 
Appropriate formulation of a stochastic optimization problem depends on the preferences of the decision-maker (DM) at hand.
We need the decision-maker's (DM's) distributional information (referred to as his `beliefs') about the underlying state of the world, as well as preferences/risk attitude (referred to as his `tastes').

In practice, there is significant and simultaneous ambiguity about both beliefs and tastes.
Ambiguity in beliefs and corresponding choice models have been a major focus of the decision theory literature, starting with min-max expected utility (MEU).
The numerical and statistical aspects of MEU have been a focus of the distributionally robust optimization (DRO) literature.
Meanwhile, ambiguity in tastes is thoroughly studied in the preference robust optimization (PRO) literature.

We want to reconcile these two types of ambiguity from both a decision-theoretic and computational point of view.
We refer to methods that simultaneously address ambiguity in both beliefs and tastes simultaneously as `distributionally preference robust optimization' (DPRO) (see \cite{hu2024distributional} which develops DPRO models for random tastes).



\subsection{Contributions}

In this paper, we seek a choice model to handle ambiguity in both DM's tastes and beliefs.
We want this model to address both types of ambiguity in a reasonable and consistent way.
So, we develop new DPRO models, their behavioral axiomatizations, and representations.
We also want our choice model to be computationally practical, so we develop efficient computational methods to use in optimization.
We now detail the specifics of our contributions.

\paragraph*{Model}
We work in the setting of Anscombe-Aumann (AA) acts (see, e.g., \cite{gilboa2016ambiguity}), which separates DM's risk preferences over objective probability distributions (lotteries) from his ambiguity preferences over unknown states of the world.
In our model, the state space accounts for both objective material states which influence probabilities, and DM's mental states which just influence his risk evaluations.
DM's uncertain tastes are embedded into the mental state.
Here, ambiguity about beliefs and tastes can be expressed in terms of ambiguity over the underlying augmented material plus mental state.
We propose a state-dependent MEU model, where there is ambiguity over the joint distribution for material and mental states.
These preferences account for ambiguity in both beliefs and tastes.

\paragraph*{Axioms and Representation}
We identify the behavioral axioms for our new choice model. The key axioms are adaptations of the classical monotonicity, independence, and uncertainty aversion axioms to our setting.
Then, we derive the representation of our preferences based on these axioms.

\paragraph*{Computation}
We show how to use our new choice model in practical stochastic optimization problems. We find a tractable numerical scheme to solve the resulting problem efficiently.

\paragraph*{Social Welfare}
We apply our framework to a problem in social welfare.


\subsection{Organization}

This paper is organized as follows.
In Section~\ref{sec:related}, we discuss the related work which motivates our current study.
In Section~\ref{sec:set-up} we review notation and our basic setting.
In Section~\ref{sec:model}, we present our choice model, its axiomatization, and its representation. We also detail how to use our choice model in practical optimization.
The following Section~\ref{sec:application} applies our framework to a social welfare problem. This section also discusses the issue of preference elicitation and develops specific examples of ambiguity sets for both beliefs and tastes.
Section~\ref{sec:discussion} discusses several specializations of our model.
The paper concludes in Section~\ref{sec:conclusion}, and all proofs appear in the Appendix.

\subsection{Notation}

For an integer $N \geq 1$, let $[N] \triangleq \{1,\ldots, N\}$ be the running index. Let $\geq$ over $\R^n$ be the standard componentwise partial order:
for all $a,b \in \R^n$, $a \geq b$ when $a_i \geq b_i$ for all $i \in [n]$. We let $\text{cl}(A)$ to denote the closure of an arbitrary set $A$.


\section{Related Work}
\label{sec:related}

In this section we review the literature on DRO, PRO, and DPRO. This literature is extensive so we just mention those works most directly connected to the present study.

\paragraph{DRO.}
DRO addresses ambiguity in the underlying beliefs (probability distribution over states of the world).
\cite{gilboa1989maxmin} develops an axiomatic foundation for min-max expected utility (MEU) for AA acts over all possible priors, to reflect uncertainty aversion. Their representation result asserts the existence of an affine utility function on acts, and a closed convex set of priors.
Notably, this model is based on a weakening of the independence assumption to reflect ambiguity aversion.
\cite{casadesus2000maxmin} develop an MEU representation and axioms for Savage acts.

More general ambiguity preferences have been studied.
\cite{maccheroni2006ambiguity} introduce an ambiguity index, which gives rise to the class of variational preferences.
See \cite{gilboa2016ambiguity} for a recent survey on ambiguity preferences.

Some risk-aware DRO models have been considered. \cite{natarajan2010tractable} studies robust expected utility, where the investor maximizes worst-case expected utility over an ambiguity set of distributions. These ambiguity sets are defined in terms of moments of the distribution. Lower bounds for the worst-case expected utility are found using conic programming, which are as tight as possible.
There is an analogous notion of worst-case risk.
Robustification of convex risk measures is considered in \cite{wozabal2014robustifying}, defined as the worst-case risk over neighborhoods of a reference probability measure. This setup leads to computationally tractable robust optimization problems. \cite{bernard2023robust} considers worst and best case distortion risk measures under distributional ambiguity.

\paragraph{PRO.}
PRO is based on ambiguity in DM's tastes, where the probabilities are known.
\cite{maccheroni2002maxmin} is among the first works to characterize ambiguity in the DM's utility function. This work is based on unclear evaluation of different outcomes (while exogenous probabilities are known). It can be interpreted in terms of `multiple selves', where the most pessimistic self is the one that makes the utility evaluation.
\cite{armbruster2015decision} develops tractable formulations for the worst-case expected utility over an ambiguity set of utility functions. This work is extended to preference ambiguity with monetary risk measures in \cite{delage2018minimizing}. \cite{cai2023distributionally} develops PRO for distortion risk measures.

Recently, Hu et al.~\cite{hu2024distributional} develop distributional utility preference robust optimization (DUPRO), where the DM's ambiguous preferences are treated as random.
Here, random utility functions model preferences in different states of the world, and potentially inconsistent preferences. 
Preference ambiguity in this setting can then be modeled as distributional ambiguity over DM's preferences.
\cite{hu2024distributional} further shows how to construct the distributional ambiguity set from available observations.
Stochastic utility functions have also appeared in the context of choice theory. \cite{jarrow2021concavity} considers a state-dependent expected utility model, where the utility function is random and there are objective probabilities defined on a separate probability space.
Our present work builds on Hu et al.~\cite{hu2024distributional} to allow for ambiguity over both beliefs and tastes through distributions on our specialized construction of the underlying state space which reflects both material and mental states.

\paragraph{DPRO.}
Existing work has already connected PRO and DRO in various ways. \cite{dentcheva2010} develops a notion of robust stochastic dominance, where second order stochastic dominance is required to hold for a set of probability models.
\cite{peng2022data} build on \cite{dentcheva2010} by combining PRO with data-driven DRO based on the Wasserstein distance around the empirical distribution. \cite{peng2022data} provide numerical algorithms to efficiently solve this class of problems.
\cite{haskell2016ambiguity} extends \cite{armbruster2015decision} to model ambiguity about both the DM's utility function and the underlying probability distribution.

While MEU was introduced to describe ambiguity in beliefs in the 1990s, \cite{hill2019non} recently extended MEU to the state-dependent setting. In \cite{hill2019non}, ambiguity in beliefs, ambiguity in tastes, and state-dependent utility are all combined.
\cite{wang2023preference_state} considers PRO with state-dependent preferences where tastes are modeled by distortion risk measures, and there is ambiguity about the underlying probability distribution.

\paragraph{Multiple Selves.}
Several works treat DM's internal/psychological state as part of the choice model. This perspective effectively means that a DM has 'multiple selves', and each self corresponds to different and possibly inconsistent preferences.
We use the concept of 'multiple selves' as the mechanism to encode ambiguity in tastes.
\cite{karni2016theory} is the most closely related work in this regard. They suppose that DM's choices depend on randomly selected 'states of mind' (mental states), where each state of mind corresponds to a different subjective expected utility (SEU) representation.
They explain stochastic choice through a random mental state that captures DM's tastes and beliefs.

\cite{hara2019coalitional} takes the perspective of multiple `selves' of the DM, and develops coalitional expected utility theory.
In this work, the DM's preferences over lotteries are represented by a coalition of utility functions, where each coalition represents a different self corresponding to a different psychological state. 

\cite{cohen2008dynamic} develops a choice model where risk evaluation depends on DM's past experiences, and the past experiences are explicitly modeled as the state space. The objects of choice here are past experience-dependent lotteries.
In contrast to our model, they suppose that payoff evaluation does not depend on the state - but DM's distortion of the distributions of lotteries does.
\cite{etner2024dynamic} develops a dynamic choice model under ambiguity where DM's attitude is allowed to depend on a history of `neutral events' that do not directly influence probabilities but influence DM's mood.



\section{Set-up}
\label{sec:set-up}

Let $\mcx \triangleq [\underline{x}, \bar{x}] \subset \Re$ be a compact set of consequences that we interpret as monetary prizes with $\sigma$-algebra $\cal B(\mcx)$.
Let $\Delta(\mcx)$ be the subset of all probability measures with finite support on the measurable space $(\mcx, \cal B(\mcx))$. We call $\Delta(\mcx)$ the set of \emph{lotteries} on $\mcx$, and write a generic lottery as $p \in \Delta(\mcx)$.
Furthermore, we let $\delta_{x} \in \calpx$ for any $x \in \mcx$ denote the degenerate lottery that returns $x$ with certainty.

We work in the standard setting of Anscombe-Aumann (AA) acts.
For now, we refer to the state space as the `material' state space to distinguish it from the `mental' state space we introduce later.
Let $\mcs$ be the underlying material state space, where $s \in \mcs$ denotes a particular material state. The following assumption holds for the rest of the paper.

\begin{assumption}
    \label{assumption:material_state}
The material state space $\mcs = \{s_1, \ldots, s_K\}$ is finite.
\end{assumption}
\noindent
Assumption \ref{assumption:material_state} is common in most preference models based on AA acts, we discuss continuous $\mcs$ in Appendix~\ref{sec:continuous}.

Let ${\cal P}(\mcs)$ be the set of all probability distributions on $\mcs$, with respect to the power set ${\cal B}(\mcs)$ of $\mcs$.
We interpret a prior $\psi \in {\cal P}(\mcs)$ as a representation of the beliefs of the decision maker (DM) about the underlying state of the world.
Each $\psi \in {\cal P}(\mcs)$ is determined by the vector of its probability masses $\psi \equiv (\psi(s_1),\ldots,\psi(s_K)) \in \R^K$, so $\calps$ is equivalent to the unit simplex in $\R^K$.

An AA act is a mapping $f:\mcs \to \Delta(\mcx)$. We let $\F$ stand for the set of all AA acts equipped with the product topology ${\cal P}(\mcx)^\mcs$. 
For an act $f \in \F$, let $f(s, x)$ denote the probability of realizing prize $x \in {\cal X}$ under lottery $f(s)$. For an event $E \in {\cal B}(\mcs)$, we define the $f_E g \in {\cal F}$ where $f_E g(s) = f(s)$ for $s \in E$ and $f_E g(s) = g(s)$ for $s \in \mcs \setminus E$. 
Furthermore, given any $f \in \F$ we let $f^s$ to denote a constant AA act where $f^s(s') = f(s)$ for all $s' \in \mcs$. Here we also mention that
a probability distribution over $\mcx$ is an constant AA act, which outputs the same lottery in every state. 

DM's preferences over ${\cal F}$ are represented by a binary relation $\succeq$. We let $\sim$ and $\succ$ denote indifference and strict preference based on $\succeq$.
A preference functional $V : {\cal F} \rightarrow \R$ is a representation of $\succeq$ where $f \succeq g$ if and only if $V(f) \geq V(g)$.

Our upcoming representations depend on utility functions to express DM's tastes.
The classical preference models separate beliefs through distributions on the state space and tastes through utility functions on consequences.
%
\begin{defn}
\label{eq:utility_function}
Let $\mathscr{U}({\cal X})$ be the set of all continuous nonconstant utility functions $u : {\cal X} \rightarrow \R$, equipped with the supremum norm. \wh{Do they need to be continuous on ${\cal X}$?} \bb{Yes, Hill imposes continuity on $u$.}
\end{defn}
%
\noindent
Given $u \in \mathscr{U}({\cal X})$ and $f \in \F$, $u(f(s)) \triangleq \sum_{x \in {\cal X}} u(x) f(s, x)$ is the expected utility of the lottery $f(s) \in \Delta(\mcx)$ in state $s$ with respect to $u$.
The expected utility of the act $f \in {\cal F}$ with respect to $\psi \in \calps$ is then $\mathbb{E}_{\psi}[u(f)] = \sum_{s \in \mcs} \psi(s) u(f(s))$.

DM's preference relation $\succeq$ on ${\cal F}$ induces a preference $\succeq_{\Delta(\mcx)}$ on lotteries $\Delta(\mcx)$.
For $p, q \in \Delta(\mcx)$, we have $p \succeq_{\Delta(\mcx)} q$ iff $f \succeq g$ where $f(s) = p$ and $g(s) = q$ for all $s \in \mcs$.
Constant acts (lotteries) do not depend on the ambiguity over the underlying state space $\mcs$, so the relation $\succeq_{\Delta(\mcx)}$ fully specifies DM's risk preferences over objective probabilities.


\section{Model}
\label{sec:model}

In this section we present our choice model which accounts for ambiguity in both beliefs and tastes. The resulting choice model can be viewed as a form of state-dependent preferences.

The material state space will continue to account for physical phenomena which actually influence objective probabilities (as it usually does). 
We defined $\mcs$ to specifically denote the material states of nature which influence objective probabilities in $\Delta(\mcx)$.
We interpret $\mcs$ as physical/environmental states (e.g., the state of the economy, market demand, weather) that can be evaluated objectively.
We further suppose that the state in $\mcs$ is fully observable.

Now, we augment the state space to also account for DM's ``mental state''. The mental (or psychological) state only influences the evaluation of objective risks without influencing their probability distributions.
We additionally and separately model DM's mental state which reflects his psychology, current mood, and past experiences.
This approach is based on \cite{karni2016theory}, which studies stochastic choice behavior of DMs in the SEU framework.
This interpretation has been used in several works where tastes are modeled by a multitude of utility functions, corresponding to a group of decision makers - or to the same DM in different states of the world.

We let $\Omega$ denote DM's mental state space, where $\omega \in \Omega$ denotes a generic mental state.
States in $\Omega$ do not influence objective probabilities, they only influence DM's risk evaluations.
We assume the mental state space is finite.

\begin{assumption}
    \label{assumption:mental_state}
    The mental state space $\Omega = \{\omega_1,\ldots,\omega_L\}$ is finite.
\end{assumption}
\noindent
We emphasize that the states $\omega \in \Omega$ `are' DM's preferences.
Alternatively, they can be interpreted as subjective signals that influence DM's risk evaluations but not objective probabilities.
Furthermore, DM's preferences may be inconsistent in different mental states.
We let ${\cal B}(\Omega)$ be the power set of $\Omega$.

We append DM's mental state to $\mcs$ to obtain the augmented state space $\mcs \times \Omega$, and we denote a generic augmented state as $(s,\omega) \in \mcs \times \Omega$.
Here, the material state $s$ is the component of the state that influences objective probabilities and $\omega$ is the component that influences DM's tastes and risk evaluation.
We let ${\cal P}(\mcs \times \Omega)$ be the set of all probability distributions on $\mcs \times \Omega$.
Additionally, we let ${\cal B}(\mcs \times \Omega)$ be the power set of $\mcs \times \Omega$.

We let ${\cal H}\triangleq\{h:\Omega \to \F\}$ denote the set of mental acts, which we equip with the product topology. Each $\omega \in \Omega$ characterizes
a mental state, i.e., a psychological state of the DM like a good/bad mood. A mental act $h \in {\cal H}$ maps each mental state $\omega$ to an AA act.
The intrinsic evaluation of prizes is different under each mood.
Mental acts are meaningful and comparable because they have observable impacts on DM's choices.

For $h \in {\cal H}$, $f \in \F$, and $\omega \in \Omega$, let $\omix{f}{h} \in {\cal H}$ be defined by $\omix{f}{h}(\tilde{\omega}) = f$ for $\tilde{\omega} = \omega$, and $\omix{f}{h}(\tilde{\omega}) = h(\tilde{\omega})$ otherwise.

\begin{example}[Vacation]
You are deciding where to go on a trip, the beach or the lake. If you go to the beach and the weather is sunny, you can swim and if you go to the lake
and the weather is rainy, you can fish.
If you are in a good mood, you prefer the beach; while if you are in a bad mood,
you prefer the lake. If you know your future mental state then you can choose optimally now while you make your travel plans. 
However, your future mental state may change based on many experiences between now and your trip. 
The choice of destination must be a constant mental act which maps every mental state to the same AA act.
From weather forcast, we know that the weather is either sunny or rainy with probability $0.6$ and $0.4$ respectively.
We let $\Omega = \{\omega_G,\omega_B\}$ denote the mental states. The material states is $\mcs = \{s_S,s_R \}$, where $s_S,s_R$ denotes sunny and rainy day.
And AA acts set is $\F = \{f_B, f_L  \}$, where $f_B$ and $f_L$ denotes going to the beach and the lake respectively.
So the outcome space $\mcx = \{ nothing, fishing, swimming \}.$ We can see that $f_B(swimming, s_S) = 0.6, f_B(nothing, s_S)= 0, f_B(nothing,s_R) = 0.4, f_B(fishing,s_R) = 0$, and
$f_L(swimming, s_S) = 0, f_L(nothing, s_S)= 0.6, f_L(nothing,s_R) = 0, f_L(fishing,s_R) = 0.4$. And suppose your evaluation over outcomes under different moods we have:
$u(swimming, \omega_G) = 2,u(fishing,\omega_G) = 1, u(nothing,\omega_G) = 0$ and $u(swimming, \omega_B) = 0,u(fishing,\omega_B) = 1, u(nothing,\omega_B) = 3$.
We can have the following payoof table:
\begin{table}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
Acts / States & Good mood, Sunny $(\omega_G, s_S)$ & Good mood, Rainy $(\omega_G, s_R)$ & Bad mood, Sunny $(\omega_B, s_S)$ & Bad mood, Rainy $(\omega_B, s_R)$ \\
\midrule
Beach $(f_B)$ & Swimming $(u=2)$ & Nothing $(u=0)$ & Nothing $(u=0)$ & Nothing $(u=0)$ \\
Lake $(f_L)$ & Nothing $(u=0)$ & Fishing $(u=1)$ & Nothing $(u=0)$ & Fishing $(u=1)$ \\
\bottomrule
\end{tabular}
}
\caption{Payoff Table for Vacation Example}
\label{tab:payoff}
\end{table}

\wh{This example could be improved by adding random weather, so that we are evaluating different AA acts in different mental states. Can we make a simple payoff table for (good weather, bad weather), and (beach, lake) for different mental states (good mood, bad mood).}
\end{example}

\wh{Some further examples could help, like: (i) capital investment (long term investment where we have to decide how much to invest in the face of uncertain yield and demand); (ii) a more specific portfolio optimization example; (iii) another group decision-making example, like a teacher setting up policies for a class.} 

Our model is based on the interaction of three different preference relations. First, we have the preference relation $\pf$ over AA acts $\F$. This preference relation captures DM's preferences over AA acts in the decision-making stage, where he faces ambiguity over both his (future) mental state and the underlying material state.
We emphasize that $\pf$ is different from the following preference over mental acts, since DM's objects of choice are AA acts.
In other words, $\pf$ is the preference relation DM uses to \textbf{compare choices}.
Second, we have the preference relation $\pfs$ over mental acts ${\cal H}$.
Though mental acts are psychological, $\pfs$ exists because different mental acts are comparable for the same DM. In other words, $\pfs$ is part of how DM \textbf{describes himself}. Finally, we have a preference relation $\pf_\omega$ over $\F$ for each mental state $\omega \in \Omega$. This preference characterizes DM's evaluation of AA acts when he knows his mental state exactly. So, $\pf_\omega$ can be viewed as a degenerate case of $\pf$.
Conversely, $\pf$ can be viewed as an aggregation of $\{\pf_\omega\}_{\omega \in \Omega}$ through $\pfs$. We call $\pfo$ \textbf{certain mental preference}, as it models preference over AA acts for a known mental state.
Conversely, we call $\pf$ \textbf{uncertain mental preference}, since it models preference over AA acts when the mental state is uncertain. Finally, we call $\pfs$ \textbf{ambiguity-averse mental preference} to emphasize that it is a preference relation on ${\cal H}$ and that it is ambiguity-averse.

Next we introduce a new class of state-dependent utility functions that account for the mental state. They are the first ingredient to our representation of $\pf$.

\begin{defn}
\label{defn:state_utility function}
Let $u : {\cal X} \times \Omega \rightarrow \R$ be a mental state-dependent utility function where: (i) $u(\cdot, \omega) \in \mathscr{U}({\cal X})$ for all $\omega \in \Omega$; and (ii) $\omega \mapsto u(x,\omega)$ is continuous for all $x \in {\cal X}$.
Then for $f \in \F$, $u(f(s), \omega) = \sum_{x \in {\cal X}} u(x, \omega) f(s, x)$ is the expected utility of lottery $f(s)$ in augmented state $(s, \omega)$.
\end{defn}

Now we give the representation for $\pf$, which can be viewed as a form of state-dependent MEU preferences (like \cite{hill2019non}).

\begin{defn}
\label{defn:model}
Let $u : {\cal X} \times \Omega \rightarrow \R$ be a mental state-dependent utility function, and let ${\cal C} \subset {\cal P}(\mcs \times \Omega)$ be a compact convex set of priors on the augmented state space.
Then let $V : {\cal F} \rightarrow \Re$ be the preference functional defined by:
%
\begin{equation}
\label{eq:model}
V(f) = \min_{\vartheta \in {\cal C}} \sum_{\omega\in \Omega}\sum_{s \in \mcs} \vartheta(s,\omega) \sum_{x\in \mcx}u(x,\omega)f(x,s).
\end{equation}
%
\end{defn}
\noindent
In Eq.~\eqref{eq:model}, the material state $s$ only enters through the probabilities of $f(s)$ and the mental state $\omega$ only enters through the state-dependent utility function.
The set $ \mcs \times \Omega$ itself is rectangular (so it imposes no requirements on admissible pairs $(s, \omega)$).
Any coupling between these states enters through the prior $\vartheta \in {\cal P}(\mcs \times \Omega)$.
For instance, the external state may influence DM's mental state (e.g., if the market is strong, the DM may feel more positive and be more risk seeking).

Eq.~\eqref{eq:model} accounts for ambiguity in both beliefs and tastes under our construction of the mental/material state space.
All ambiguity faced by DM over both beliefs/tastes in this model is captured by the ambiguity about the distribution $\vartheta \in {\cal P}(\mcs \times \Omega)$ on the underlying augmented state space $\mcs \times \Omega$.
DM's `multiple selves' enter this model through the random mental state $\omega$.




\subsection{Axiomatization}
\label{sec:axiomatization_mental_state}

In this section, we take $\pfs$ as the primitive preference relation in our model upon which the other two preference relations are based.
From $\pfs$, we will determine DM's preferences over $\F$ for the decision-making stage.

Here we introduce some basic axioms for $\pfs$.
%
\begin{itemize}
    \item[A1] (Completeness) For all $h, h' \in {\cal H}$, either $h \pfs h'$ or $h' \pfs h$.
    \item[A2] (Transitive) If $h, h', h'' \in {\cal H}$ and $h \pfs h'$ and $h' \pfs h''$, then $h \pfs h''$.
    \item[A3] (Nondegeneracy) There are $h, h' \in {\cal H}$ such that $h \succ^* h'$.
    \item[A4] (Continuity) For all $h \in {\cal H}$, the sets $\{h' \in {\cal H} : h \pfs h'\}$ and $\{h' \in {\cal H} : h \preceq^* h'\}$ are closed in the product topology on ${\cal H}$.
\end{itemize}
%

Axioms [A1] - [A4] appear widely in the literature, we refer to them as the ``Basic Axioms''.

Let $h \in {\cal H}$ and $\omega \in \Omega$, then $h^{\omega} \in {\cal H}$ is defined by $h^{\omega}(\tilde{\omega}) = h(\omega)$ for all $\tilde{\omega} \in \Omega$, i.e., it is a constant mental act. For $f \in \F$ and $s \in \mcs$, let $f^s(\tilde{s}) = f(s)$ for all $\tilde{s} \in \mcs$, i.e., it is a constant AA act.
Then, $f_\omega^s h (\omega)= f(s)$ and $f_\omega^s h(\omega') = h(\omega')$ for all $\omega' \in \Omega\setminus\{\omega\}$, and $g_\omega^s h(\omega) = g(s)$ and $g_\omega^s h(\omega') = h(\omega')$ for all $\omega' \in \Omega\setminus \{\omega\}$.

The `Monotonicity' axioms need to be carefully applied in our framework due to the presence of multiple preference relations.

\begin{itemize}
    \item[A5] (Monotonicity) Let $h, h' \in {\cal H}$ and suppose $h^\omega \pfs {h'}^\omega$ for all $\omega \in \Omega$, then $h \pfs h'$.
    \item[A5'] (State Monotonicity) For all $f,g \in \F$ and for all $\omega \in \Omega$, if $f_\omega^s h \pfs g_\omega^s h$ for all $s \in \mcs,$ then $f_\omega h \pfs g_\omega h$.
\end{itemize}

Next we extend the notion of state independence to mental acts.

\begin{itemize}
    \item[SI] (State Independence) For all $h,h' \in {\cal H}$, $f,g \in \F$, and $\omega, \omega' \in \Omega$, $f_\omega h \pfs g_\omega h$ if and only if $f_{\omega'} h' \pfs g_{\omega'} h'$.
\end{itemize}
Under `State Independence', if one AA act $f$ mixed with a mental act $h$ is better than another AA act $g$ mixed with the same mental act $h$, then DM  always prefers $f$ in every mental state.  
According to \cite{hill2019non}, under the standard Independence axiom [I], (see Appendix~\ref{sec:appendix_axioms}), Monotonicity is equivalent to State Independence.
However, state independence prevents tastes from differing across mental states.
Hence, we adopt ``State Monotonicity'' here and do not require Monotonicity or State Independence. 

To continue, we recall two crucial axioms which first appeared in \cite{gilboa1989maxmin} to establish the foundation for the well-known MEU model.
For any given mental state $\omega$, the following two axioms are necessary for $\pf_\omega$ to have a MEU representation for preferences over $\F$.
%
\begin{itemize}
    \item [C-I] (Certainty Independence) For all $f,g \in \F$, $p \in \Delta(\mcx)$ and $\alpha \in (0,1)$, $f \pf_\omega g$ if and only if
    $\convmix{f}{p} \pf_\omega \convmix{g}{p}.$
    \item [UA] (Uncertainty Aversion) For all $f,g \in \F$ and $\alpha \in (0,1)$, if $f \sim_\omega g$, $\convmix{f}{g} \pf_\omega f$.
\end{itemize}
%
[C-I] means that DM's preference will not change if two AA acts are mixed with the same constant act. [UA] means that, given two equally preferred uncertain prospects (AA acts), DM prefers their mixture to either one.
We state these axioms for $\pfo$ in line with \cite{gilboa1989maxmin}, since the original [UA] and [C-I] axioms are for preferences on $\F$. We will show that each $\pfo$ has an MEU representation, and then we obtain our overall representation for $\pf$ by aggregating them. 

The following State Consistency axiom is a modification of the one in \cite{hill2019non} to account for mental states. It captures the idea that a DM's preference over AA acts under a fixed $\omega$ should be consistent across different mental states.

%
\begin{itemize}
    \item [SC] (State Consistency) For each $h \in {\cal H}$ and $f,g \in \mathcal{F}$, if $\omix{f}{h}\pfs \omix{g}{h}$ for all $\omega \in \Omega$, then $\omix{f}{h'} \pfs \omix{g}{h'}$ for all $h' \in {\cal H}$.
\end{itemize}
%

We need a way to compare AA acts, i.e. $f\in \F$, under different mental states.
We want to identify the largest set $\mathcal{G} \subset {\cal H}$ such that $h \pfs h'$ if and only if $\alpha h + (1-\alpha) g \pfs \alpha h' + (1-\alpha) g$ for all $\alpha \in [0,1]$ and $g \in \mathcal{G}$.
This is a type of independence condition.
These are the mental acts that can be mixed with other mental acts without changing the preference.

In \cite{gilboa1989maxmin} where the DM has state-independent preferences, $\mathcal{G}$ is the set of constant acts.
However, the preferences in our model are state-dependent. In our case, a different mental state may yield different utility for the same act.

In our model, $\mathcal{G}$ is determined by the $\pfs$-best-and-worst mental acts, defined as follows.
This definition is based on the state consistency axiom.

\begin{defn}[Best and worst mental acts]
    \label{defn:best-worst}
    $\overline{h},\underline{h} \in {\cal H}$ are \emph{$\pfs$-best-and-worst mental acts}, respectively, if for every $\omega \in \Omega$ and every $h \in {\cal H}$, we have:
    \begin{enumerate}
        \item $\omix{\overline{h}}{h} \pfs \omix{f}{h} \pfs \omix{\underline{h}}{h}$ for all $f \in \F$;
        \item $\omix{\overline{h}}{h} \succ^* \omix{\left(\convmix{\overline{h}}{\underline{h}}\right)}{h}$ for all $\alpha \in [0, 1)$.
    \end{enumerate}
    \end{defn}

The \emph{$\pfs$-best-and-worst} acts yield the highest and lowest possible ``evaluation'' in each mental state $\omega$, with certainty. The existence of (not necessarily unique) \emph{$\pfs$-best-and-worst} acts is guaranteed by the following result.

\begin{lem}[Proposition 1 in \cite{hill2019non}]
    \label{lem:best-worst}
    If $\pfs$ satisfies [A1] - [A4] and [SC], then there exist $\overline{h},\underline{h} \in {\cal H}$ that are $\pfs$-best-and-worst.
\end{lem}

\cite{hill2019non} defines the strong uncertainty aversion axiom, which reflects the intrinsic ambiguity aversion of a DM.
We modify this axiom to cover mental acts.

%
\begin{itemize}
    \item[SUA] (Strong Uncertainty Aversion with respect to $\overline{h}, \underline{h}$) For all $h,h' \in {\cal H}$ and $\alpha, \beta, \gamma \in [0,1]$,
    if $h \pfs \left(\beta \overline{h} + (1-\beta) \underline{h}\right)$ and $h' \pfs \left(\gamma \overline{h} + (1-\gamma) \underline{h}\right)$, 
    then $$\convmix{h}{h'} \pfs \left(\convmix{\left(\beta \overline{h} + (1-\beta)\underline{h}\right)}{\left(\gamma \overline{h} + (1-\gamma) \underline{h}\right)}\right).$$ 
    \item [EC-I] (EC-Independence with respect to $\olh,\ulh$) For all $h,h' \in {\cal H}$, $\alpha \in [0,1]$, and $\beta \in(0,1)$, $h \pfs h'$ if and only if 
    $$
    \convexmix{h}{\left(\convmix{\olh}{\ulh}\right)}{\beta} \pfs \convexmix{h'}{\left(\convmix{\olh}{\ulh}\right)}{\beta}.
    $$
\end{itemize}
%

Axioms [SUA] and [EC-I] generalize the axioms of the standard MEU model, as shown in \cite{hill2019non}. We modified these two axioms to the setting of mental acts. [SUA] strengthens classical uncertainty aversion. Under [EC-I], convex combinations of $\pfs$-best-and-worst mental acts play a similar role as  constant acts do in the MEU model.

The following two axioms [SWUA] and [SC-I] correspond to DM's preferences over AA acts under a fixed mental state $\omega$ (i.e., they correspond to $\pfo$). 
%
\begin{itemize}
    \item [SWUA] (State-wise Uncertainty Aversion) For all $h \in {\cal H}$, $f,g\in \F$, $\alpha \in [0,1]$, and $\omega \in \Omega$, if $\omix{f}{h}\sim^* \omix{g}{h}$, then $\omix{\left(\convmix{f}{g}\right)}{h} \pfs \omix{f}{h}.$
    \item [SC-I] (State Certainty Independence) For all $h \in {\cal H}$ and $\alpha \in (0,1)$, $f,g\in \F$, $p\in \Delta(\mcx)$, and $\omega \in \Omega$, if $\omix{f}{h} \pfs \omix{g}{h}$, then $\omix{\left(\convmix{f}{p}\right)}{h} \pfs \omix{\left(\convmix{g}{p}\right)}{h}.$ 
\end{itemize}
%

Under [SWUA], in any fixed mental state $\omega$, DM prefers mixtures of two equally preferred acts. This axiom expresses DM's desire to hedge against the uncertainty over material states. [SC-I] is an extension of the standard [C-I] axiom from \cite{gilboa1989maxmin}.


\subsection{Representation}

We derive the representation of $\pfs$ in this subsection, which describes DM's preferences over mental acts. We first define an $h,h'$-precise representation.

Next we define the specifics of the desired representation for $\{\pfo\}$ and $\pfs$.

\begin{defn}
\label{defn:representation_pfs}
(i) For $\omega \in \Omega$, $\pfo$ has an MEU representation on $\F$ if there exists a non-empty, closed, and convex set $\Psi_\omega \subset {\cal P}(\mcs)$ such that $f \pfo g$ for $f,g \in \F$ if and only if $v(f,\omega) \geq v(g,\omega)$, where:
\begin{equation}
\label{eq:preference_omega_representation}
v(f,\omega) = \min_{\psi \in \Psi_\omega} \sum_{s \in \mathcal{S}} \psi(s) \sum_{x \in \mcx}u(x,\omega)f(x,s).
\end{equation} 


(ii) $\pfs$ has a mental MEU representation on ${\cal H}$ if there exists a non-empty, closed, and convex set $\Phi \subset {\cal P}(\Omega)$ such that $h \pfs h'$ for $h,h' \in {\cal H}$ if and only if $V(h) \geq V(h')$ where:

\begin{equation}
\label{eq:preference_star_representation}
V(h) = \min_{\phi \in \Phi} \sum_{\omega \in \Omega}\phi(\omega)v(w,h(\omega)).
\end{equation}
In addition, $V:{\cal H}\to \R$ is $h,h'$-precise: for any $h,h' \in {\cal H}$ and any $\alpha \in [0,1]$, we have $V(\convmix{h}{h'}) = \convmix{V(h)}{V(h')}$.
\end{defn}

Eq.~\eqref{eq:preference_star_representation} models DM's MEU preferences over mental acts, given the fact that he faces ambiguity over his future mental state.
Eq.~\eqref{eq:preference_omega_representation} models a DM's MEU preferences over AA acts under each fixed mental state $\omega$.
The proof of our main result is based on aggregating $\pfo$.

Our main theorem presented next is based on the following idea. For each $\omega$, $\pfo$ is a preference over AA acts and has an MEU representation. We aggregate all of these representations into a single augmented MEU representation for $\pf$ on the augmented state space.

\begin{thm}
\label{thm:mental_DPRO}
The following statements are equivalent:

(i) $\pfs$ satisfies Axioms [A1] - [A4], [A5'], [SC], [SUA], [EC-I], [SWUA], and [SC-I].

(ii) $\pfs$ has a mental MEU representation.
\end{thm}

\begin{defn}
\label{defn:representation_pf}
$\pf$ has a mental MEU representation on $\F$ if there exists a non-empty, closed, and convex set ${\cal C} \subset {\cal P}(\mcs \times \Omega)$ such that $f \pf g$ for all $f,g \in \F$ if and only if $V(f) \geq V(g)$ where:
\begin{equation}
\label{eq:preference_representation}
V(f) = \min_{\vartheta \in \mathcal{C}} \sum_{\omega\in \Omega}\sum_{s \in \mcs} \vartheta(s,\omega) \sum_{x\in \mcx}u(x,\omega)f(x,s),
\end{equation}
and $V$ is $\olh,\ulh$-precise. \wh{here we need to check the use of $\olh,\ulh$-precise, because this $V$ is a mapping on $\F$.}
\end{defn}

Here we mention that, our $V:\F \to \R$ is a restriction of $V:\mathcal{H}\to \R$, whose domain is $\F$. Thus $V$ still satisfies the fact that 
$V$ is $\olh,\ulh$-precise.

The following theorem gives the representation for $\pf$ over AA acts. AA acts are a special case of mental acts (i.e., they are constant mental acts), so the representation for $\pf$ follows from the axioms for $\pfs$.

\begin{thm}
\label{thm:DPRO}
The following statements are equivalent:

(i) $\pfs$ satisfies Axioms [A1] - [A4], [A5'], [SC], [SUA], [EC-I], [SWUA], and [SC-I].

(ii) $\pf$ has a mental MEU representation on $\F$.

\end{thm}


\subsection{Optimization}

Now we apply our choice model to stochastic optimization.
Let ${\cal Z} \subset \R^{N_z}$ be the set of feasible decisions, and let $\Xi \subset \R^{N_{\xi}}$ be the support for the problem uncertainty.
Let $\xi(s) \in \Xi$ denote the uncertainty as a function of the underlying state $s \in \mcs$.
Given any $s \in \mcs$, the distribution of $\xi(s)$ is known objectively, even though we are uncertain about which underlying state will be realized.
Next, we define $r : {\cal Z} \times \Xi \rightarrow \R$ to be the reward function which depends on $(z, \xi)$ (it only indirectly depends on the underlying state through the distribution of $\xi(s)$).

We make the following technical assumptions.
%
\begin{assumption}
\label{assu:optimization}
(i) ${\cal Z} \subset \R^{N_z}$ is convex and compact.

(ii) $\Xi \subset \R^{N_\xi}$ is convex and compact.

(iii) $z \mapsto r(z, \xi)$ is continuous and concave for all $\xi \in \Xi$.

(iv) $\xi \mapsto r(z, \xi)$ is continuous for all $z \in {\cal Z}$.
\end{assumption}
%

We to decide on our decision variable $z \in \mathcal{Z}$, where $\xi(s)$ is the random vector which is ubiquitous in stochastic optimization problems.
But in our setting, we set AA acts $f : \mcs \to \Xi$, which means for every $s \in \mcs$, $f(s)$ is a degenerate distribution with $\mathbb{P}\left(\xi = \xi(s)\right) =1$. Thus we have 
the following optimization problem.
%
\begin{equation}
\label{eq:opt}
\max_{z \in {\cal Z}} V(r(z)) \equiv \max_{z \in {\cal Z}} \min_{\vartheta \in {\cal C}} \sum_{(\omega,s) \in (\Omega, \mcs)} \vartheta(\omega,s) u\left(r\left(z, \xi(s)\right), \omega\right).
\end{equation}
%

\bb{
We can also set the material state space as $\Xi$. Then we will directly have that:
$$
\max_{z \in {\cal Z}} V(r(z)) \equiv \max_{z \in {\cal Z}} \min_{\vartheta \in {\cal C}} \sum_{(\omega,\xi) \in (\Omega, \Xi)} \vartheta(\omega,\xi) u\left(r\left(z, \xi\right), \omega\right).
$$

}

\wh{We need to establish that this problem is convex, and identify an efficient way to solve it. Can we easily differentiate the objective, and use a first-order algorithm? Can we get a tractable reformulation of the entire problem when $\Omega$ and $\mcs$ are finite? These issues need to be addressed here.
We should be able to show that $V$ is concave in acts $f \in \F$ as the minimum of linear functions. Then, the composite objective is convex, and the above is a convex optimization problem.
}

\begin{thm}
\label{thm:optimization}
Problem~\eqref{eq:opt} is a convex optimization problem if and only if utility functions $x \mapsto u(x,\omega)$ is concave in $x$.
\end{thm}

In Problem \ref{eq:opt}, clearly we know it is a convex optimization problem for $z$ since the inner problem is concave in $z$, then the whole problem is
convex in $z$. 


\section{Application}
\label{sec:application}
In this section we develop some examples of our choice model by identifying specific ambiguity sets for beliefs and tastes, and constructions of state-dependent utility functions $u(\cdot, \omega)$.
Our setup is general and allows for several different constructions of $u$ as a function of the mental state.
We discuss how to construct ${\cal C}$ and $u$ in practice (especially for finite $\mcs$).

Suppose you are setting a policy that will affect the welfare of a population, e.g., a tariff policy. The population is heterogeneous and different people have different preferences over policies. We can represent each population member as a mental state. Some want to protect domestic industries, while others want to promote free trade.
    However, you can not make a policy that is different for each person. As the policy maker, you need to make a ``constant'' mental act.

\subsection{Mechanism Design - Social Allocation}
In the 1960s, Fishburn \cite{fishburn1969preferences} first attempted to construct a ``social choice function'' to aggregate the preferences of a group of people through summation of prefernces.
In \cite{keeney1975group}, Keeney studied maximizing the social welfare of a group of people in terms of the group's expected utility.
This maximization depends on balancing efficiency and fairness.

Suppose the DM is allocating resources among a group of agents to maximize the social welfare.
There are $L$ types of agents and each agent has her own preferences over allocation outcomes.
We model the $L$ types of agents through their mental states $\Omega =\{ \omega_1,\ldots,\omega_L\}$. 
In addition, there are $K$ material states $\mcs = \{ s_1,\ldots,s_K\}$ which will influence the outcomes of DM's allocation. 
The space of outcomes is an interval $\mcx \subset \R$.
We let $f \in \F$ denote a feasible allocation policy, which maps each material state $s \in \mcs$ to a probability distribution over outcomes $\mcx$.
The DM has partial information about how a typical agent will evaluate allocation outcomes. For a given policy and a realized material state, an agent with type $\omega$ will evaluate the allocation policy by:
$$
u(f(s),\omega) = \sum_{x \in \mcx} u(x,\omega) f(x,s).
$$

DM does not know the distributions of material states nor of agent types. We let $\vartheta \in {\cal P}(\mcs \times \Omega)$ denote a prior distribution on material states and types of agents. DM's ambiguity set for priors is denoted by ${\cal C}$.
DM wants to maximize social welfare based on our DPRO model and solve:
$$
\sum_{\omega\in \Omega} \sum_{s\in \mcs} \vartheta(s,\omega) \sum_{x \in \mcx} u(x,\omega) f(x,s).
$$
In this example, the DM's optimal allocation policy $f^* \in \F$ is defined by the following definition.
\begin{defn}[Efficient and fair allocation rule]
An allocation rule $f^*$ is optimal (with respect to efficiency and fairness) when:
\begin{equation}
    \label{eq:optimal_allocation}
    f^* \in \arg \max\limits_{f \in \F} \min\limits_{\vartheta \in {\cal C}} \sum_{\omega\in \Omega} \sum_{s\in \mcs} \vartheta(s,\omega) \sum_{x \in \mcx} u(x,\omega) f(x,s).
\end{equation}
\end{defn}  

We use our DPRO model to achieve fairness in a robust way. We consider every agent type and their preferences over consequences. We optimize against the most pessimistic distribution, which gives our fairness guarantee.

\subsection{State-Dependent Utility}

\begin{example}
Let $\mcs = \{s_1, \ldots, s_K\}$ be finite where each state $s_k$ corresponds to a different utility function $u_k$. This model is non-parametric, no assumption about the relationship between $u_k$ and $u_{k'}$ is needed.
\end{example}

\begin{example}
(Piecewise linear) Let $a = x_0 \leq x_1 \leq \cdots \leq x_J = b$ be a partition of the domain $[a, b]$ of the utility functions.
We suppose $u(a) = 0$ and $u(b) = 1$ without loss of generality since translation and normalization do not affect DM's preferences.

We introduce the variables $t_j = u(x_j) - u(x_{j-1})$ for the increment of $u$ on partition $j \in [J]$, and we let $w = (t_1, \ldots, t_J) \in \R^J$.
We can construct piecewise linear $u$ in the following additive form:
%
\begin{equation}
\label{eq:sum_utility}
u(x; w) = \sum_{j \in [J]} \left[ \frac{t_j}{x_j - x_{j-1}}(x - x_{j-1}) + \sum_{k=1}^{j-1} t_k \right] 1\{x_{j-1} < x \leq x_j\}.
\end{equation}
%
The mental state is the parameter $w \in \R^J$. We can construct polyhedral ${\cal W}$ to represent increasing utility functions and increasing concave utility functions.

For $u(\cdot; w)$ to be increasing and concave, $w$ must belong to the set ${\cal W} \subset \R^J$ defined by:
%
\begin{align*}
    \sum_{j \in [J]} t_j = & 1,\\
    (x_j - x_{j-1})t_{j+1} \leq & (x_{j+1} - x_{j}) t_j,\, \forall j \in [J-1],\\
    w \geq & 0,
\end{align*}
%
corresponding to normalization, concavity, and  monotonicity, respectively.
\end{example}

\begin{example}
(Piecewise linear concave) Suppose we have $J$ affine functions where $t_j = (t_{j1}, t_{j2})$ give the slope and intercept, respectively, for affine function $j \in [J]$.
Then let $w = (t_1, \ldots, t_J) \in \R^{2 J}$.
We have
%
\begin{equation}
\label{eq:min_utility}
u(x; w) = \min_{j \in [J]} \langle t_j, (x, 1) \rangle.
\end{equation}
%
Again, the mental state is $w$. By setting the state space to be:
$$
{\cal W} = \{w \in \R^{2 J} : t_{j1} \geq 0,\, \forall j \in [J]\},
$$
we recover all non-decreasing concave functions.
\end{example}

We can also represent utility functions as probability distributions (see, e.g., \cite{hu2012robust}).

\begin{example}
Let $\hat{u} \in {\cal U}({\cal X})$ be a reference utility function.
Then, for any other $u \in {\cal U}({\cal X})$ we can write
$$
u(p) = \sum_{x \in {\cal X}} p(x) u(x).
$$
\end{example}


\subsection{Distributional Information}

Now we give some specific examples of belief/taste ambiguity sets on $\calppsi$.
Above we defined the marginals $\vartheta_s$ and $\vartheta_w$.
We can encode separate partial information on beliefs and tastes through the sets $\vartheta_s \in {\cal C}_s$ and $\vartheta_w \in {\cal C}_t$.

We also have partial information that couples beliefs and tastes.
First, we have preference elicitation constraints $f_j \succeq g_j$ for all $j \in [J]$ which are expressed as 
$$
\bbe_{\vartheta}[u(f_j(s); w)] \geq \bbe_{\vartheta}[u(g_j(s); w)],\, \forall j \in [J].
$$
In particular, these are linear constraints on $\vartheta$.

We also have certainty equivalent bounds ${\rm CE}(h_k) \in [c_k^l, c_k^u]$ for all $k \in [K]$, where the certainty equivalent ${\rm CE}(h_k) = u^{-1}(\bbe_{\vartheta}[u(h_k)])$ depends on $(\vartheta, u)$.

We can incorporate additional requirements on $v$ through preference elicitation to obtain a strict subset ${\cal V} \subset \mathscr{V}$. Suppose $W_i \succeq Y_i$ under probability model $P_i$ for $i \in [I]$, then we have $\mathbb{E}_{P_i}[u(W_i; v)] \geq \mathbb{E}_{P_i}[u(Y_i; v)]$ for all $i \in [I]$ which is a linear inequality in $v$.

\subsection{Eliciting the Ambiguity Set}

We need to elicit two inputs from DM to use our model. First, we need the utility function $u(\cdot,\omega)$ for each agent type.
But in our model, we have no ambiguity over utility functions (agents' preferences). What we need to recover is our ambiguity set of priors ${\cal C}$.

\subsubsection{Eliciting Tastes}

Our background is we did some survey and found there are $L$ types of agents, in which people have the same answers. We suppose through survey data that we can recover the tastes for each agent.
And we let $\omega_l$ to denote each type.

\begin{assumption}
Each agent type with preference $\omega_l$ has utility function $u(\cdot,\omega_l) \in \mathcal{U}$.
\end{assumption}

Let ${\cal U} = \{ u(\cdot, \omega_1),\dots, u(\cdot,\omega_L)\}$ be the collection of agents' utility functions.

\subsubsection{Confidence Bounds}

We use a lower bounding method to recover the ambiguity set of priors.
Let $C = \{ {c_m} \}_{m \in [M]}$ be a collection of AA acts.
For agent type $l \in [L]$, we suppose the utility of $c_m$ is lower bounded by $\underbar{c}_{ml}$.
Then, agents of type $l$ have the ambiguity set:
$$
\Psi_l =\{ \psi_l \in {\cal P}(\mcs): v(c_m,\omega_l) \geq \underline{c}_{ml}, \forall m \in [M] \},
$$
which is explicitly:
\begin{equation}
    \underline{c}_{ml} \leq \min_{\psi_l \in \Psi_l} \sum_{s\in \mcs}\psi_l(s) \sum_{x \in \mcx} u(x,\omega_l)c_m(x,s), \quad \forall m \in [M].
\end{equation}

For each agent type, the ambiguity set $\Psi_l$ is a convex set given by supporting hyperplane theorem.
We let $\Phi(\omega)$ and $\Psi(s|\omega)$ to denote marginal distributions of $\vartheta$ over $\Omega$ and $\mcs$, respectively. 
Finally, we let $\mathcal{C}_{CB}$ denote the ambiguity set based on all agents' confidence bound comparisons given by:
\[
\mathcal{C}_{CB} = \bigg\{ \vartheta \in \mathcal{P}(\Omega \times \mcs): \vartheta(\omega_l,s) = \phi(\omega_l) \psi(s|\omega_l), \psi(s|\omega_l) \in \Psi_l, \forall l \in [L], s\in\mcs.\bigg\}
\]
\wh{why is normalization needed below, shouldn't all $\vartheta$ already be well-defined probability distributions?}
$$
\mathcal{C}_{CB} = \left\{ \vartheta \in \mathcal{P}(\Omega \times \mcs):  \underline{c}_{ml} \leq  \sum_{s\in \mcs} \frac{\vartheta(\omega,s)}{\sum_{s'}\vartheta(\omega,s')} \sum_{x \in \mcx} u(x,\omega_l)c_m(x,s), \quad \forall m \in [M] , \forall l \in [L]\right\}.
$$


\subsubsection{Ambiguity set of priors}
From the survey data, we can derive the empirical data over $\Omega$. And for each type of agents, we also cloud have their empirial estimations for material states. We can formulate an ambiguity set around the empirical distribution. Next we recall the definition of the KL-divergence.

\begin{defn}[KL-divergence]
    For discrete probability distributions $P$ and $Q$ over measurable space $(Y,{\cal B}(Y))$, the KL-divergence is defined as:
    \[
    D_{KL} (P,Q)=  \sum_{y\in Y} P(y) \log \left( \frac{P(y)}{Q(y)} \right).
    \] 
\end{defn}

Here we denote the empirical joint distribution as $\hat{\vartheta}(\omega,s) = \hat{\phi}(\omega) \hat{\psi}(s|\omega).$ Let $\epsilon > 0$ and define the KL-divergence based ambiguity set:
\[
\mathcal{C}_{KL} = \big\{ \vartheta \in {\cal P}(\Omega \times \mcs): D_{KL}(\vartheta, \hat{\vartheta}) \leq \epsilon \big\}.
\]

Based on the above discussion, we can construct our ambiguity set by:
\[
\mathcal{C} = \mathcal{C}_{CB} \cap \mathcal{C}_{KL}.
\]

We establish that the ambiguity set $\mathcal{C}$ is convex in the following proposition.
\begin{prop}
    \label{prop:_ambiguity_convex}
    The ambiguity set $\mathcal{C}$ is convex.
\end{prop}
\begin{proof}[Proof of Proposition \ref{prop:_ambiguity_convex}]
    It is obvious that $\mathcal{C}_{KL}$ is convex. And $\mathcal{C}_{CB}$ is also convex since it is actually intersection of halfspaces. Since $\mathcal{C}$ is 
    the intersection of two convex sets, it is also convex.
\end{proof}

We have now fully specified the ambiguity set for our DPRO model. We can now formulate the optimization problem for social welfare maximization. In the following, we can derive a convex conic programming problem. We start with the following definitions.

\begin{defn}
\label{defn:cone}
(i) A conic inequality $\preceq_K$ induced by a proper cone $K$ is defined as:
$$
x \preceq_K y \Leftrightarrow y - x \in K.
$$

(ii) Let $K$ be a cone. The set 
\[
K^* =\left\{ y\, | \, x^T y\geq 0 \text{ for all } x\in K \right\}
\]
is called the dual cone of $K$.
\end{defn}

\begin{defn}[Exponential cone]
\label{defn:Exponential_cone}
The exponential cone $K_\text{exp}$ is defined as: 
$$K_\text{exp} = \text{cl}\left(\left\{x \in \Re^3:x_1 \geq x_2 e^{x_3/x_2},x_2>0 \right\}\right).$$
\end{defn}

The dual cone to $K_\text{exp}$ is (see, e.g., \cite{serrano2015algorithms}):
$$
(K_{\text{exp}})_* = \text{cl}\left( \left\{  s\in \Re^3: s_1 \geq -s_3 e^{(s_2-s_3)/s_3},s_3 <0\right\} \right).
$$

Recall that $X$ is the space of consequences and $\boldsymbol{z} \in \R^{N_\mathcal{Z}}_+$ is our decision vector. We consider the following policy optimization problem:
Suppose there are $N_\mathcal{Z}$ baseline polycies $f_1,f_2,...,f_{N_\mathcal{Z}}$ and a mapping $r: \R^{N_\mathcal{Z}}_+ \to \F$ which is linear in $\boldsymbol{z}$. Now $\boldsymbol{z}$ is a weight
vector for the baseline policies. We can write $r(\boldsymbol{z}) = \sum_{i=1}^{N_\mathcal{Z}} z_i f_i$.

\begin{thm}
\label{thm:convex_formulation}
Suppose $r:\R^{N_\mathcal{Z}}_+ \to \F$ is linear in $\boldsymbol{z}$. With a little abuse of notation, we let $u(\boldsymbol{z},\omega,s):= \sum_{x\in \mathcal{X}}u(x,\omega)r(\boldsymbol{z})(x,s)$.
We have the following convex optimization problem:
    \begin{align*}
        \max_{\boldsymbol{z}} \min_{\boldsymbol{\vartheta},\boldsymbol{\delta}} & \sum_{\omega,s}\vartheta(\omega,s) u(\boldsymbol{z},\omega,s) \\
        \text{s.t.}  \quad & \underline{c}_{m,\omega} \leq \sum_{s} \frac{\vartheta(\omega,s)}{\sum_{s'\in \mcs}\vartheta(\omega,s)} u(c_m,\omega,s) \quad \forall m \in [M], \omega \in \Omega, \\
        & \sum_{s,\omega} \vartheta(\omega,s) = 1, \\
        & \sum_{\omega,s} \delta_{\omega,s} \leq \epsilon, \\
        & \sum_{i=1} ^{N_\mathcal{Z}} z_i =1 \\
        & \begin{bmatrix}
            -\hat{\vartheta}(\omega,s) \\
            -\vartheta(\omega,s) \\
            \delta_{\omega,s}
        \end{bmatrix} \preceq_{K_{\text{exp}}} \begin{bmatrix}
            0 \\
            0 \\
            0
        \end{bmatrix} \quad \forall \omega \in \Omega, s\in \mcs,\\
        & \boldsymbol{z} \in \R^{N_\mathcal{Z}}_+, \boldsymbol{\vartheta} \in \R_+^{|\Omega|\times |\mcs|}, \boldsymbol{\delta}\in \Re^{|\Omega|\times |\mcs|}.
    \end{align*}

This problem is equivalent to the following conic linear programming problem: 

    \begin{align*}
        \max_{\boldsymbol{z},\boldsymbol{\alpha},\beta,\gamma, \boldsymbol{\lambda}} & -\beta - \epsilon \gamma - \sum_{\omega,s} \hat{\vartheta}(\omega,s)\lambda_1(\omega,s)\\
    \text{s.t.} \quad &u(\boldsymbol{z},\omega,s) - \sum_m \alpha_{m,\omega} \left( \underline{c}_{\omega,m} - u(c_m,\omega,s)\right) + \beta - \lambda_2(\omega,s) \leq 0 \quad \forall \omega,s\\
    & \gamma + \lambda_3(\omega,s) = 0 \quad \forall \omega,s\\
    & \sum_{i=1}^{N_\mathcal{Z}} z_i = 1 \\
    & \boldsymbol{z}\in\R^{N_\mathcal{Z}}_+ ,\boldsymbol{\alpha} \in \R_+^{M\times |\Omega|}, \beta , \gamma \in \Re, (\lambda_1(\omega,s), \lambda_2(\omega,s),\lambda_3(\omega,s))\in \left(K_{\text{exp}}\right)_*.
    \end{align*}
\end{thm}

\subsection{Numerical Expirements}
In this section, we conduct numerical expirement to demonstrate the performance of our DPRO model. We still consider simple example of social wealfare models.
Firstly, we set the ourcome space $\mcx = \{ 0,1,2,3,4 \}$. Also in this section, we consider different number of agents $L$ and material states $K$.
And we compare our results with MEU models.

Since in our DPRO model, we have amgibuity over types of agents, we assume each type of agents, $\omega_l$, has power utility function with different risk aversion attitudes $\gamma_l$, i.e.,
$$
u(x,\omega_l) = \frac{x^{1-\gamma_l}}{1-\gamma_l}, \quad  \gamma_l \in (0,1).
$$

While in MEU model, we assume all agents have the same power utility function with the same risk aversion attitude. To proceed, we consider different number of
$L$ and $K$ to see how our model performs. 

We evaluate the performance of our model by out-sample ``total social welfare'', which is the summation of all agents' utility given different realized sample of mental states and material states.
We assume there exists an underlying real distribution over mental states $\phi^*$. And we sample 100 people from the population. Thus, we can have a 
empirical distribution $\hat{\phi}$ over mental states. And we also assume similarly real and empirical distribution $\psi^*$ and $\hat{\psi}$ over material states.

At the beginning of the experiment, we generate a sample with size 1000 under the real distribution $\phi^*$. From this realized sample, we recover the empirical distribution 
$\hat{\phi}$ over mental states for DPRO model. And we generate some random sample from real distribution $\psi^*$ to get the empirical distribution $\hat{\psi}$ over material states.

Then we solve the optimization problem in Theorem \ref{thm:convex_formulation} where we set $\epsilon = \bb{undecided}$ and each agent type has a different risk aversion attitude $\gamma_l$.
And for MEU model, we set the utility function as power utility function with weighted risk aversion attitude $\gamma_{MEU} = \bbe_{\hat{\phi}}[\gamma_l]$. The original problem of MEU is:
\begin{align*}
    \max_{\boldsymbol{z}} \min_{\psi} & \sum_{s} \psi(s) u(\boldsymbol{z},s) \\
    \text{s.t.} & \sum_{s} \psi(s) = 1, \\
    & \sum_{s} \delta_{s} \leq \epsilon, \\
    & \sum_{i=1} ^{N_\mathcal{Z}} z_i =1 \\
    & \begin{bmatrix}
        -\hat{\psi}(s) \\
        -\psi(s) \\
        \delta_{s}
    \end{bmatrix} \preceq_{K_{\text{exp}}} \begin{bmatrix}
        0 \\
        0 \\
        0
    \end{bmatrix} \quad \forall s\in \mcs,\\
    & \boldsymbol{z} \in \R^{N_\mathcal{Z}}_+, \boldsymbol{\psi} \in \R_+^{ |\mcs|}, \boldsymbol{\delta}\in \Re^{ |\mcs|}.
\end{align*}

And we can write its dual convex conic programming problem as:
\begin{align*}
    \max_{\boldsymbol{z},\;\beta,\;\gamma,\;\{\lambda_s\}}\quad &
    -\;\beta\;-\;\epsilon\,\gamma
    \;-\;\sum_{s}\hat{\psi}(s)\,\lambda_{1}(s)
    \\[6pt]
    \text{s.t.} & u\bigl(\boldsymbol{z},s\bigr)\;+\;\beta\;-\;\lambda_{2}(s)\;\;\le\;0
    \quad\forall\,s,\\
    &\quad \gamma\;+\;\lambda_{3}(s)\;=\;0\quad\forall\,s,\\
    &\quad \sum_{i=1}^{N_z} z_i\;=\;1,\quad z \in \mathbb{R}_{+}^{N_z},\\     
    &\quad \gamma \;\ge\; 0, \beta\;\in\;\R ,(\lambda_{1}(s),\lambda_{2}(s),\lambda_{3}(s))\;\in\;(K_{\mathrm{exp}})^{*}.
\end{align*}

From above 2 optimization problems, we can derive the optimal policies for DPRO and MEU models, $z^*_{DPRO}$ and $z^*_{MEU}$. Then given the real distribution $\phi^*$ and $\psi^*$, 
we can generate random samples from them to evaluate out-of-sample total social welfare under optimal policies.

\section{Discussion}
\label{sec:discussion}

In this section we discuss connect our model with the related literature. We also consider some special cases of our choice model that arise from restricting some axioms.


\subsection{Ambiguous Tastes Only}

We can specialize our model for ambiguous tastes only. In this setting, all probabilities are objectively known so we drop $\mcs$ and just work on $\Omega$.
This aligns with the setting of \cite{hu2024distributional}, where the DM's utility function is random but there is no ambiguity about objective probabilities. 

We continue to suppose that the mental state does not influence consequences (only their utility evaluation). Then we just need to evaluate constant mental acts where $h(s,\omega) = p = h(s',\omega)$ for some $p \in \calpx$ for all $s ,s'\in \mcs$ and $\omega \in \Omega$.
In other words, all states now correspond to the same objective probabilities and only differ in their risk evaluations.

Let $V : \calpx \rightarrow \R$ be defined by:
%
\begin{equation}
\label{eq:tastes_only}
V(p) = \min_{\vartheta \in {\cal C}} \sum_{\omega \in \Omega} \vartheta(\omega) \sum_{x\in \mcx} u(x,\omega) p(x).
\end{equation}
%
Only ambiguity in mental states appears in Eq.~\eqref{eq:tastes_only}. 
We introduce the following axiom for this setting:
%
\begin{itemize}
    \item[S-I] (State Independence) For all $h\in {\cal H}$ and $\alpha \in (0,1)$, $f,f', f'' \in \F$ and all $\omega \in \Omega$, if we have
    $\omix{f}{h} \pfs \omix{f'}{h}$ then $\omix{\left(\convmix{f}{f''}\right)}{h} \pfs \omix{\left(\convmix{f'}{f''}\right)}{h}$.
\end{itemize}
%

\begin{thm}
\label{thm:tastes_only}
The following statements are equivalent:

(i) $\pfs$ satisfies Axioms [A1] - [A4], [A5'], [SC], [SUA], [EC-I] and [SI].

(ii) There exists a set of distributions ${\cal C} \subset \mathcal{P}(\Omega)$ and a subjective prior $\psi \in \calps$ such that $p \pfs q$ if and only if $V(p) \geq V(q)$ where $V : \calpx \rightarrow \R$ is given by Eq.~\eqref{eq:tastes_only}.
\end{thm}

In the ambiguous tastes only setting, we only have ambiguity about the probability distribution of mental states $\omega$. meanwhile, under each mental state,
the DM has a unique subjective prior $\psi_\omega$ over the material state space $ \mcs$. Since it is an evaluation function over constant AA acts, it drops the prior
about material states.

\subsection{Ambiguous Beliefs Only}

When only beliefs are ambiguous, we drop ${\cal W}$ and just work on $\mcs$.
Consider
%
\begin{equation}
\label{eq:beliefs_only}
V(f) = \min_{\vartheta \in {\cal C}} \sum_{s \in \mcs} \vartheta(s) u(f(s); s),
\end{equation}
%
which only has imprecision in beliefs (with state-dependent utility), for the PRO setting.
Eq.~\eqref{eq:beliefs_only} is known as state-dependent maxmin expected utility (MEU).

\begin{thm}
\label{thm:beliefs_only}
The following statements are equivalent:
    
(i) $\succeq$ satisfies Axioms [A1] - [A5], [Cvx], and [CI]. 

(ii) There exists a set of distributions ${\cal C} \subset \calps$ such that $f \succeq g$ if and only if $V(f) \geq V(g)$ where $V$ is given by Eq.~\eqref{eq:beliefs_only}.
\end{thm}
\noindent
This result is originally established for the state-independent case in \cite{gilboa1989maxmin}.



\subsection{Pessimistic Tastes}

\cite{hill2019non} develops a general model for AA acts ($f \in \F$) evaluation under state-dependent utility with imprecision in both beliefs and tastes given by
%
\begin{equation}
\label{eq:hill}
V(f) = \min_{\vartheta \in {\cal C}} \sum_{s \in \mcs} \vartheta(s) \min_{u \in v(s)} u(f(s)),
\end{equation}
%
where ${\cal C}$ is still a set of priors on $\mcs$ and now ambiguity in tastes is captured by the multi-function $v : \mcs \rightrightarrows {\cal U}({\cal X})$.

\cite{hill2019non} introduces the concept of ``impression'' of taste by allowing a DM to have a set of utility functions in each state.
Our model differs from this concept, we shut down the impression of tastes so DM has a precise utility function in every state. Our ambiguity over tastes is expressed by the ambiguous prior on mental states. In \cite{hill2019non}, the impression of tastes is given by the ambiguity of utility functions.

To generalize  the main representation result from \cite{hill2019non} (see \cite[Theorem 1]{hill2019non}). We introduce some axioms and notations for our representation theorem:
%
\begin{itemize}
    \item [SC*] (State Consistency*) For each $\omega \in \Omega$, $h,h'\in {\cal H}$, $f,g \in \F$, $p,q \in \calpx$ and every $s\in \mcs$, if $\omix{(p_s f)}{h} \pfs \omix{(q_s f)}{h}$, we have
    $\omix{(p_s g)}{h'} \pfs \omix{(q_s g)}{h'}$.
\end{itemize}

Here we mention that [SC*] is a stronger version of [SC], since [SC] is guaranteed if [SC*] holds. To proceed, we need to introduce state dependent preferences, see Definition \ref{defn:state_dep_preference}. And since
$\pfo$ is a binary relation over $F$, similar to best-and-worst mental acts, see Definition \ref{defn:best-worst}, we can define best and worst AA acts under mental state $\omega$.
\begin{defn}[Best-and-worst AA acts]
\label{defn:best-worst_AA_acts}
$\olf[\omega],\ulf[\omega] \in \F$ are $\pfo$-best-and-worst if for every $s\in \mcs$ and every $f\in \F$ we have:
\begin{enumerate}
    \item $\olf[\omega]_s f \pfo \smix{p}{f} \pfo \smix{\ulf[\omega]}{f}$ for all $p \in \calpx$;
    \item $\smix{\olf[\omega]}{f} \succ_\omega \smix{\left(\convmix{\olf[\omega]}{\ulf[\omega]}\right)}{f}$ for all $\alpha \in [0,1).$
\end{enumerate} 
\end{defn}

Given this definition, guaranteed by Lemma \ref{lem:best-worst} we have for each $\omega$, there exists such $\olf[\omega],\ulf[\omega]$ which are best-and-worst AA acts under preference $\pfo$.

Then we can introduce the following axiom:
%
\begin{itemize}
    \item[SUA*] (Strong Uncertainty Aversion with respect to $\olf[\omega],\ulf[\omega]$) For all $f,f' \in \F$ and $\alpha, \beta, \gamma \in [0,1]$,
    if $f \pfo \left(\beta \olf[\omega] + (1-\beta) \underline{f}[\omega]\right)$ and $f' \pfo \left(\gamma \overline{f}[\omega] + (1-\gamma) \underline{f}[\omega]\right)$, 
    then $$\convmix{f}{f'} \pfo \left(\convmix{\left(\beta \overline{f}[\omega] + (1-\beta)\underline{f}[\omega]\right)}{\left(\gamma \overline{f}[\omega] + (1-\gamma) \underline{f}[\omega]\right)}\right).$$ 
    \item [EC-I*] (EC-Independence with respect to $\olfo,\ulfo$) For all $f,f' \in \mathcal{F}$, $\alpha \in [0,1]$, and $\beta \in(0,1)$, $f \pfo f'$ if and only if 
    $$
    \convexmix{f}{\left(\convmix{\olfo}{\ulfo}\right)}{\beta} \pfo \convexmix{f'}{\left(\convmix{\olfo}{\ulfo}\right)}{\beta}.
    $$
\end{itemize}

\begin{thm}
\label{thm:MEU_Pessimistic_MEU}
    Let $\pfs$ be a binary relation on ${\cal H}$ and for each $\omega \in \Omega$, $\pfo$ is defineed as Definition \ref{defn:state_dep_preference}. The following statements are equivalent:

    (i) $\pfs$ satisfies Axioms [A1] - [A4], [SC*], [SUA] and [EC-I] and each $\pfo$ satisfies [SUA*] and [EC-I*].
    
    (ii) There exists a non-trivial, closed, convex, $\olf,\ulf$-constant, $\olf,\ulf$-precise function $v:\Omega \times \mcs \to 2^{\mathbb{U}}\setminus \emptyset$ and a null-consistent,
    closed, convex set of priors ${\cal C} \in {\cal P}(\Omega \times \mcs)$ such that $f \pfs g$ if and only if $V(f) \geq V(g)$ where $V$ satisfies:
    \[
    V(f) = \min_{\vartheta \in {\cal C}} \sum_{\omega \in \Omega} \sum_{s \in \mcs} \vartheta(\omega,s) \min_{u \in v(\omega,s)} u\big(f(s)\big).
    \]
\end{thm}

One of the key distinctions between Eq.~\eqref{eq:model} and Eq.~\eqref{eq:hill} is that tastes are random under Eq.~\eqref{eq:model}, while Eq.~\eqref{eq:hill} assumes the most pessimistic evaluation of a lottery for each state.
As argued in \cite{hu2024distributional}, random utility gives the modeller and DM more flexibility in expressing preferences. In contrast, the term $\min_{u \in v(s)}u\left(f(s)\right)$ which appears in Eq.~\eqref{eq:hill} always assumes the most pessimistic evaluation of a lottery in a given state. We can recover Eq.~\eqref{eq:hill} as a special case of Eq.~\eqref{eq:model} through appropriate choice of the priors ${\cal C}$.

We can reformulate Eq.~\eqref{eq:hill} as
%
\begin{equation}
\label{eq:hill-1}
V(f) = \min_{\vartheta \in {\cal C}} \sum_{s \in \mcs} \vartheta(s) \min_{\omega \in v(s)} u(f(s); \omega),
\end{equation}
%
to facilitate comparison with Eq.~\eqref{eq:model}.
Now, the multi-function $v : \mcs \rightrightarrows {\cal W}$ maps from external states to subsets of the mental states.

\begin{thm}
\label{thm:hill_equivalence}
Suppose Eq.~\eqref{eq:hill} is given by $({\cal C}, v)$, and let ${\cal C}' \in \calppsi$ be defined by
$$
{\cal C}' = {\rm conv}\{{\cal C}\}
$$
Then, Eq.~\eqref{eq:model} for ${\cal C}'$ and Eq.~\eqref{eq:hill} are equivalent.
\end{thm}
\noindent
In light of this result, Eq.~\eqref{eq:model} can be viewed as more flexible and less conservative than Eq.~\eqref{eq:hill}.

There is also a distinction between Eq.~\eqref{eq:model} and Eq.~\eqref{eq:hill} in terms of preference elicitation.
Eq.~\eqref{eq:hill} requires a multi-utility $v(s)$ to be given for every state. When the number of states is large or uncountable, the preference elicitation step becomes extremely challenging.
In contrast, the preference elicitation step for Eq.~\eqref{eq:model} is completely contained in the construction of ${\cal C}$.






\section{Conclusion}
\label{sec:conclusion}

This work is motivated by the problem of choice under ambiguity in both beliefs and tastes. Our framework is based on the intuition of DM's multiple selves, which represent possibly inconsistent beliefs and tastes.
We develop a variation of the AA setup where the state space accounts for the objective material states as well as DM's different mental states corresponding to his multiple selves.
Our approach treats both material states and mental states as random, and encompasses an uncertainty set of probability distributions on augmented states.
Furthermore, this approach can be viewed as extending minimax expected utility (MEU) preferences to the DPRO setting.

Our DPRO framework is also applicable to problems like mechanism design, where the controller must account for the ambiguity of other agents' beliefs and tastes.



\bibliographystyle{plain}
\bibliography{References}


\newpage
\appendix

\section{Table of Notation}

\begin{longtable}[c]{>{\raggedright}p{0.2\textwidth} p{0.75\textwidth}}
    \hline
    \textbf{Symbol} & \textbf{Description} \\
    \hline
    \endfirsthead
    
    \hline
    \textbf{Symbol} & \textbf{Description} \\
    \hline
    \endhead
    
    \hline
    \endfoot
    
    \hline
    \endlastfoot
    
    $\mcs$ & Set of all material states, where $\mcs=\{s_1,\ldots,s_{K}\}$. \\
    $\Omega$ & Set of all mental states, where $\Omega=\{\omega_1,\ldots,\Omega_{L}\}$. \\
    $\mcx$ & Outcome space, which is compact.\\
    $\Delta(\mcx)$ & Set of all lotteries on space $\mcx$.\\
    ${\cal P}(\cdot)$ & Set of all Borel probability measure on measurable space $\cdot$.\\
    $f/\F$ & AA act and AA acts set. $f:\mcs \to \calpx$.\\ 
    $f,g$ & Typical AA acts.\\
    $h/{\cal H}$ & Mental act and mental acts set. $h:\Omega\to \F$.\\
    $h,h'$ & Typical mental acts.\\
    $p,q,r$ & Typical lotteries, which is also ``constant act''.\\
    $\mathcal{C},\vartheta$ & Ambiguity set over $\Omega \times \mcs$.\\
    $\Psi_\omega,\psi$ & Ambiguity set over $\mcs$ in different $\omega$.\\
    $\Phi,\phi$ & Ambiguity set over $\Omega$ \\
    $\pf$ & Mental uncertainty preference. DM's preference relation over $\F$, when he has ambiguity over mental and material states.\\
    $\pfs$ & Ambiguity-averse mental acts preference. DM's preference relation over ${\cal H}$\\
    $\pf_\omega$ & Mental certain preference. DM's preference relation over AA acts $\F$ when he only has ambiguity over material states.\\

\end{longtable}

\section{Basic Axioms}
\label{sec:appendix_axioms}
\begin{itemize}
    \item [I] (Independence) For all $h,h',h'' \in {\cal H}$ and $\alpha \in [0,1]$, if $h \pfs h'$, then $\convmix{h}{h''} \pfs \convmix{h'}{h''}$.
\end{itemize}

\section{Additional Material for Section~\ref{sec:model} (Model)}

\subsection{Proof of Theorem ~\ref{thm:mental_DPRO}}
Here we prove our main result, which underpins our DPRO model.
\subsubsection{Proof of sufficiency: $(i) \implies (ii)$}
First we prove that $(i) \implies (ii)$.

We begin our proof with a preliminary definition and lemma. The next definition formalizes the state-dependent preferences induced by $\pfs$.

\begin{defn}[State dependent preference]
\label{defn:state_dep_preference}
For every mental state $\omega\in \Omega$, the relation $\pf_\omega$ is defined over AA acts $\F$ by $f\pf_\omega g$ if and only if $\omix{f}{h} \pfs \omix{g}{h}$ for some $h \in {\cal H}$.
\end{defn}

Recall Axiom [SC] from \cite{hill2019non}, where $f_\omega h\pf g_\omega h$ if and only if $\omix{f}{h'} \pf \omix{g}{h'}$ for all $h' \in {\cal H}$. Then, the above definition can be restated with 'for all $h \in {\cal H}$'.

\begin{lem}
    \label{lem:state_dep_preference_lem}
    For every $\omega$, $\pf_\omega$ is a continuous weak order.
\end{lem}
\begin{proof}[Proof of Lemma \ref{lem:state_dep_preference_lem}]
    For every $f,g \in \F$, if $g \nsucceq_\omega f$, we know $\omix{f}{h} \pfs \omix{g}{h}$ for every $h\in {\cal H}$ by [SC]. Because the definition for $\pfo$ is exist some $h$ such that $ \omix{g}{h} \pfo \omix{f}{h}$. negate it we will have for all $h$, $\omix{f}{h} \pfs \omix{g}{h}$.
    It follows that $f\pf_\omega g$, which establishes that $\pf_\omega$ is complete. 
    Next suppose $f \pf_\omega f'$ and $f' \pf_\omega f''$. Then there exist $h,h'$ such that $\omix{f}{h} \pfs \omix{f'}{h}$ and $\omix{f'}{h'} \pfs \omix{f''}{h'}$. It follows that we have $\omix{f}{h} \pfs \omix{f''}{h}$ by axiom [SC] and transitivity of $\pfs$, which shows $\pf_\omega$ is transitive.
    Finally we show that $\pf_\omega$ is continuous. For every $f \in \F$, by Axiom [SC] we have
    \begin{align*}
    \{g:g\in \F, g\pf_\omega f\} = & \{g:g\in \F, g_\omega h \pfs f_\omega h,\, \forall h \in {\cal H}\}\\
    =& \{g:g\in \F g_\omega h \pfs f_\omega h, \exists h \in {\cal H}\},
    \end{align*}
    which is since closed $\pfs$ is continuous. 
    By similar reasoning, the set $\{g:g\in \F, f\pf_\omega g\}$ is also closed. We conclude that $\pf_\omega$ is a continuous weak order.
\end{proof}

Here we recall the two crucial axioms from \cite{gilboa1989maxmin}, where the axiomatization for the MEU model is established.
\begin{itemize}
    \item [C-I] (Certainty Independence) For all $f,g \in \F$, $p \in \Delta(\mcx)$ and $\alpha \in (0,1)$, $f \pf_\omega g$ if and only if
    $\convmix{f}{p} \pf_\omega \convmix{g}{p}.$
    \item [UA] (Uncertainty Aversion) For all $f,g \in \F$ and $\alpha \in (0,1)$, if $f \sim_\omega g$, we have $\convmix{f}{g} \pf_\omega f$.
\end{itemize}

We repeat that $\pf_\omega$ is a preference relation over $\F$ for each fixed $\omega$ and we precede our proof by following lemma.
\begin{lem}
    \label{lem:state_dep_preference_axiom_lem}
    If $\pfs$ satisfies [A5'], [SWUA] and [SC-I], for all $\omega \in \Omega$, $\pfo$ satisfies [A5], [UA] and [C-I].
\end{lem}
\begin{proof}[Proof of Lemma \eqref{lem:state_dep_preference_axiom_lem}]
    First we prove that if $\pfs$ satisfies [A5'], $\pfo$ satisfies [A5]. Suppose that $f^s \pfo g^s$ for all $s \in \mcs$, then by the definition of $\pfo$, when know there exist a $h_{(\omega, s)}$ such that
    $\omix{{f^s}}{h_{(\omega, s)}} \pfo \omix{{g^s}}{h_{(\omega, s)}}$ for every $s$. And by Axiom [SC] we know there exist a common $h$ such that $\omix{{f^s}}{h} \pfs \omix{{g^s}}{h}$ for every $s$. Then guaranteed by axiom [A5'], we know
    $f \pfo g$. This proves that $\pfo$ satisfies [A5].

    Second, we prove $\pfo$ satisfies [UA]. If $f \sim_\omega g$, by definition of $\pfo$, there exist a $h$ such that $\omix{f}{h} \sim^* \omix{g}{h}$. Then by axiom [SWUA], we have for
    $\alpha \in [0,1]$, $(\convmix{f}{g})_\omega h \pfs \omix{f}{h}$. Then by definition of $\pfo$, we have $\convmix{f}{g} \pfo f$. This proves that $\pfo$ satisfies [UA].

    Finally, we prove that $\pfo$ satisfies [C-I]. If $f \pfo g$, by definition of $\pfo$, there exist a $h$ such that $\omix{f}{h} \pfs \omix{g}{h}$. Then by axiom [SC-I], we have for all $\alpha \in (0,1)$ and $p \in \Delta(\mcx)$,
    $\omix{(\alpha f + (1-\alpha)p)}{h} \pfs \omix{(\alpha g + (1-\alpha)p)}{h}$. By definition of $\pfo$, we know $\alpha f + (1-\alpha)p \pfo (\alpha g + (1-\alpha)p).$
\end{proof}

By Lemma~\ref{lem:state_dep_preference_lem},
$\pf_\omega$ is a continuous weak order. 
Now, we can obtain a representation result for $\pf_\omega$ for every $\omega$. 
Note that $\olh(\omega)$ and $\ulh(\omega)$ are best-and-worst AA acts under each mental state $\omega$. Since $\pfo$ is continuous, for any $f\in \F$ there is a unique $\alpha_f$ such that
$f \sim_\omega \convexmix{\olh(\omega)}{\ulh(\omega)}{\alpha_f}$. We prove the uniqueness of $\alpha_f$ as follows.
Assume there are $\alpha,\beta$ such that $f \sim_\omega \convexmix{\olh(\omega)}{\ulh(\omega)}{\alpha}  \sim_\omega \convexmix{\olh(\omega)}{\ulh(\omega)}{\beta}$.
Without loss of generality, we assume that $\alpha > \beta$ so that
$$
\convexmix{\olh(\omega)}{\ulh(\omega)}{\alpha} \sim_\omega \convexmix{\olh(\omega)}{\left( \convexmix{\olh(\omega)}{\ulh(\omega)}{\beta} \right)}{\frac{\alpha -\beta}{1-\beta}}.
$$
Since we have $\olh(\omega) \succ_\omega \convexmix{\olh(\omega)}{\ulh(\omega)}{\beta}$ and
$\convexmix{\olh(\omega)}{\ulh(\omega)}{\beta} \sim_\omega \convexmix{\olh(\omega)}{\ulh(\omega)}{\beta}$ (i.e., an act is indifferent with itself), by Axiom [UA], we know that:
$$
\convexmix{\olh(\omega)}{\ulh(\omega)}{\alpha} \sim_\omega \convexmix{\olh(\omega)}{\left( \convexmix{\olh(\omega)}{\ulh(\omega)}{\beta} \right)}{\frac{\alpha -\beta}{1-\beta}} \succ_\omega \convexmix{\olh(\omega)}{\ulh(\omega)}{\beta}.
$$
This is a contradiction, so $\alpha_f$ must be unique.

Then, for each $\pf_\omega$ there exists a representation $v:\F \to \R$ which is concave in its first argument.
Next, similar to the standard MEU construction (e.g., see \cite{gilboa1989maxmin}), there exists a closed convex set of distributions $\Psi_\omega \subset \calps$ for each $\omega$ such that $v(\cdot,\omega)$ takes the form:

\[
v(f,\omega) = \min_{\psi \in \Psi_\omega} \sum_{s \in \mathcal{S}} \psi(s) \sum_{x \in \mcx}u(x,\omega)f(x,s),\, \forall f \in \F.
\]
This construction is justified since $\pfo$ satisfies Basic Axioms, Monotonicity, C-I, and UA for every $\omega$.

To proceed, we introduce the following proposition here, which we will use in our further proof:
\begin{prop}[Aggregation Proposition]
    \label{prop:aggregation_proposition}
    If $\pfo$ is defined as above with valuation function $v(\omega,\cdot)$, and $\pfs$ satisfies Axioms [A1] - [A4], [SUA] and [EC-I], we have
    that representation function for $\pfs$ is given by
    \[
    V(h) = \min_{\phi \in \Phi} \sum_{\omega \in \Omega} \phi(\omega) v\left(\omega,h(\omega)\right).
    \]
\end{prop}

\begin{proof}[Proof of Proposition \ref{prop:aggregation_proposition}]
Now we return attention back to our overall preference $\pfs$ over ${\cal H}$.
Every $h \in {\cal H}$ can be expressed as a convex combination of $\pfs$-best-and-worst acts.
Since $\pfs$ satisfies Axioms [A1] - [A4] and [SUA], there exists a representation
\begin{equation}
    \label{eq:definition_V}
    V:{\cal H} \to [0,1],
\end{equation}
defined by $V(h)=\alpha_h$ where $h \sim^* \convexmix{\olh}{\ulh}{\alpha_h}$. This is similar to the construction in \cite{hill2019non}. 
Since $\alpha_h$ is unique, $V$ is well-defined. Since $\pfs$ is continuous and satisfies [SUA], we know that
$V$ is continuous and concave. Furthermore, we know that $V$ is $h,h'-$precise given the definition of $V$.

Next, let ${\cal D}$ denote the set of all simple functions on
$\Omega$, where ${\cal D}=[0,1]^\Omega$ (recall $\Omega$ is finite).
gives the vector of values for every mental state $\omega$. For every $h\in {\cal H}$, define
$\hat{V}(h)\in {\cal D}$ by $\hat{V}(h)(\omega) = v(h(\omega),\omega)$.
Then, we may define a functional $I:{\cal D}\to \R$ by $I(a) = V(h)$ such that $\hat{V}(h) =a$, for all $a\in {\cal D}$. This function is well-defined.
By Axioms [A1] and [SC] we know that 
for every $h,h' \in {\cal H}$, if $h\sim_\omega h'$ for every $\omega$, then $h\sim^* h'$. By definition, it follows that
\begin{equation}
    \label{eq:definition_I}
    V(h) = I(\hat{V}(h))
\end{equation}

Next we recall the definition of positively homogeneous and normalized functionals.
\begin{defn}
\label{defn:positve_homogeneous}
A functional $I:{\cal D} \to \R$ is positively homogeneous when, for any $a\in {\cal D}$, $I(\alpha a) = \alpha I(a)$ for all $\alpha>0$.
\end{defn}
Let $e \in {\cal D}$ be the constant function $e(\omega)=1$ for all $\omega \in \Omega$.
\begin{defn}
\label{defn:normalized}
A functional $I:{\cal D}\to \R$ is normalized when $I(z e) = z$ for all $z\in [0,1]$.
\end{defn}

We proceed with the following lemma about the representation $I$.

\begin{lem}
    \label{lem:MEU_DROP_lem1}
    $I$ is concave, continuous, monotone, positively homogeneous, and normalized.
\end{lem}
\begin{proof}[Proof of Lemma~\ref{lem:MEU_DROP_lem1}]
    Normalization is trivial by the definition of $I$. Monotonicity of $I$ is directly from Axiom [SC]. 
    We define the equivalence relation $\sim^*$ on the topological space ${\cal H}$. 
    Since $\hat{V}:{\cal H} \to {\cal D}$ is a quotient map and $V$ is continuous by Eq.~\eqref{eq:definition_V}, 
    we know $V = I \circ \hat{V}$ by Eq. \eqref{eq:definition_I}. 
    We recall the following universal property of quotient maps.
    Since $V:{\cal H}\to \Re$ is a map such that $h \sim^* h'$ implies $V(h) =V(h')$ for all $h,h'  \in {\cal H}$ and $I:{\cal H}/\sim^* \to \Re$ and $I \circ \hat{V} = V$ where $\hat{V}:{\cal H}\to {\cal H}/\sim^*$ is the quotient map. Then $I$ is continuous if and only if $V$ is continuous.
    By the universal property of quotient maps, $I$ is continuous if and only if $V$ is continuous, from which we derive that $I$ is continuous.
    
    Next we prove that $I$ is concave.
    By the previous certainty equivalence, for any $a\in {\cal D}$ we could define $h^a(\omega) \sim^* \convexmix{\olh(\omega)}{\ulh(\omega)}{a}$ where $a(\omega) \in [0,1]$ for every $\omega$.
    Then for any $a,b \in {\cal D}$, we let $h^a,h^b \in {\cal H}$ denote
    the mental acts such that $\hat{V}(h^a) = a$ and $\hat{V}(h^b) = b$. 
    For any $\alpha \in [0,1]$, we have $\hat{V}(\convmix{h^a}{h^b}) = \convmix{a}{b}$  by the above construction of $\hat{V}$ in Eq.~\eqref{eq:definition_I}. By concavity of $V$, we have
    $$
    I(\convmix{a}{b})= V(\convmix{h^a}{h^b}) \geq \convmix{V(h^a)}{V(h^b)}= \convmix{I(a)}{I(b)},
    $$
    which shows that $I$ is concave.

    To see that $I$ is positively homogeneous, let $b = \boldsymbol{0} \in {\cal D}$ where $\boldsymbol{0}(\omega) = 0$ for all $\omega \in \Omega$.
    For any $a \in {\cal D}$ and $\alpha \in [0,1]$, by Eq.~\eqref{eq:definition_V} we know that $h^b = \ulh$ since $V(h^b) = I(b) = 0$ and $V(h^b) =0$ if and only if $h^b = \ulh$. So, we have:
    $$
    I(\alpha a + (1-\alpha)b) = V(\convexmix{h^a}{h^b}{\alpha}) = \alpha V(h^a) = \alpha I(a),
    $$
    where the first equality is from Eq.~\eqref{eq:definition_I}, the second is from [EC-I], and the third is by the normalization of $I$. For $\alpha>1$, we let $c = \alpha a$. Since $I(c) = I(\alpha a) = \alpha I(a)$,
    we have
    $$
    I(\alpha^{-1}c) = I(a) = \alpha^{-1}I(c),
    $$
    which concludes the proof.
\end{proof}

The next lemma will conclude the proof of the theorem.
\begin{lem}
    \label{lem:MEU_DROP_lem2}
    If the function $I$ is normalized, concave, monotone, and positively homogeneous, then there exists a nonempty, closed, convex subset $\Phi$ of $\mathcal{P}(\Omega)$ such that for all $a\in {\cal D}$, $I(a) = \min_{\phi \in \Phi}\langle \phi,a \rangle$.
\end{lem}
\begin{proof}[Proof of Lemma~\ref{lem:MEU_DROP_lem2}]
    Let $\hat{\mathcal{D}}$ be the set of all $\mathcal{B}(\Omega)$-measurable simple functions on $\R_+^{\Omega}$.
    By positive homogeneity of $I$, we can extend $I$ to a normalized, monotone, superlinear functional $\ell$ on $\hat{\mathcal{D}}$.
    For each $a \in \hat{{\cal D}}$, the restriction of $I$ to span$(a)$ is a linear functional by positive homogeneity \wh{should it be the cone of $a$?}.
    By the Hahn-Banach extension theorem, this linear functional can be extended to a linear functional $\ell$ on ${\cal D}$ that dominates $I$, 
    see \cite[Theorem 5.53]{ali2006}. Since $I$ is monotone and normalized, $\ell$ is non-negative since $\ell(a) \geq I(a) \geq I(0) =0$, where the first inequality holds because $\ell$ dominates $I$ on ${\cal D}$, the second holds by monotonicity of $I$, and the equality holds because $I$ is normalized.
    In summary, $\ell(a) = I(a)$ if $a\in {\cal D}$ and $\ell(a) \geq I(a)$ if $a \in \hat{{\cal D}}\setminus {\cal D}$.
    Since $\ell$ is a positive, continuous linear functional, by the Riesz Representation Theorem (see, e.g., \cite[Theorem 14.5]{ali2006})
    we conclude that
    $\ell=\langle \cdot,\phi_a \rangle$ for some Borel measure $\phi_a$, where the inner product $\langle a, \phi_a \rangle$ is defined by:
    $$
    \langle a, \phi_a \rangle = \int_{\Omega} a  \, \dee \phi_a.
    $$

    Next we prove that $\phi_a$ is actually a probability measure. Note $\ell$ is a positive linear functional and we have $\ell(1) = \langle 1, \phi_a \rangle = 1$. Since $\phi_a$ is a Borel measure, it must be a probability measure.
    Let $\Phi$ be the set of all $\phi \in \mathcal{P}(\Omega)$ satisfying
    $\langle a,\phi \rangle \geq I(a)$ for all $a \in {\cal D}$. Then $\Phi$ is nonempty, closed, and convex by the Supporting Hyperplane Theorem, see \cite[Theorem 7.19]{ali2006}. 
    Finally, since $\ell(a) = I(a)$ for all $a\in {\cal D}$, we know that $I(a) = \ell(a) = \langle a, \phi_a \rangle = \min_{\phi \in \Phi} \langle a,\phi \rangle$.
\end{proof}

By Lemma~\ref{lem:MEU_DROP_lem2}, we conclude that $V$ satisfies:
\[
    V(h) = \min_{\phi \in \Phi} \sum_{\omega \in \Omega}\phi(\omega)v(w,h(\omega)),\, \forall h\in {\cal H}.
\]

\end{proof}    

The above analysis shows that statements (i) $\implies$ (ii). 

\subsubsection{Proof of necessarity: $(ii) \implies (i)$}

Next we prove that (ii) $\implies$ (i). Given the representation of $V(\cdot)$, we induce the preference relationship $\pfs$ on mental act space ${\cal H}$ by definition of for every $h,h' \in {\cal H}$, if $V(h) \geq V(h')$, we say $h \pfs h'$. We recall that $V(h)$ takes the form of:

\[
V(h) = \min_{\phi \in \Phi} \sum_{\omega \in \Omega}\phi(\omega) \min_{\psi_\omega \in \Psi_\omega} \sum_{s\in \mcs} \psi_\omega(s)\sum_{x\in \mathcal{X}}u(x,\omega)f(x,s)
\]

The basic Axioms [A1] - [A4] follows from the representation of $V(\cdot)$, since $V(\cdot)$ is continus and well-defined and non-trivial. 

Then we prove $\pfs$ induced by $V$ satisfies [A5']. To simplify our notation, we let $\hat{u}(f(s),\omega) = \sum_{x\in \mathcal{X}} u(x,\omega) f(x,s)$ for $f \in \mathcal{F}$.
If ${f^s}_\omega h \pfs {g^s}_\omega h $ for every $s$, we have for every $s$:

\begin{align*}
V\left({f^s}_\omega h\right) &= \min_{\phi \in \Phi}  \left[ \phi(\omega) \min_{\psi_\omega \in \Psi_\omega} \sum_{s\in \mcs} \psi_\omega(s)  \hat{u}(f(s),\omega) + \sum_{\omega' \in \Omega\setminus\{\omega\}} \phi(\omega') \min_{\psi_{\omega'} \in \Psi_{\omega'}}  \sum_{s\in \mcs}\psi_{\omega'}(s) \hat{u}(h(\omega',s),\omega') \right] \\
& = \min_{\phi \in \Phi} \left[ \phi(\omega) \ \hat{u}(f(s),\omega) + \sum_{\omega' \in \Omega\setminus\{\omega\}} \phi(\omega') \min_{\psi_{\omega'} \in \Psi_{\omega'}}  \sum_{s\in \mcs} \psi_{\omega'}(s) \hat{u}(h(\omega',s),\omega') \right] 
\end{align*}

The second equality holds because $\sum_{s\in \mcs} \phi_\omega (s) = 1$. By construction, we know for every $s$ we have:
\[
V({g^s}_\omega h) \geq \min_{\phi \in \Phi} \phi(\omega) \left[ \hat{u}(g(s),\omega) -   \hat{u}(f(s),\omega) \right] + V({f^s}_\omega h),
\]

which implies $\hat{u}\left( g(s),\omega\right) - \hat{u}\left( f(s),\omega\right) \leq 0$ for every $s$, since $V\left( {f^s}_\omega h\right) \geq V\left( {g^s}_\omega h \right)$. Then we have:
\begin{align*}
    V(f_\omega h) &= \min_{\phi \in \Phi}  \left[ \phi(\omega) \min_{\psi_\omega \in \Psi_\omega} \sum_{s\in \mcs} \psi_\omega(s)  \hat{u}(f(s),\omega) + \sum_{\omega' \in \Omega\setminus\{\omega\}} \phi(\omega') \min_{\psi_{\omega'} \in \Psi_{\omega'}}  \sum_{s\in \mcs}\psi_{\omega'}(s) \hat{u}(h(\omega',s),\omega') \right] \\
    & \geq \min_{\phi \in \Phi} \phi(\omega) \left[ \min_{\psi_\omega \in \Psi_\omega} \sum_{s\in \mcs} \psi_\omega(s)  \hat{u} \left(f(s),\omega\right)  -   \min_{\psi_\omega \in \Psi_\omega} \sum_{s\in \mcs} \psi_\omega(s) \hat{u} \left(g(s),\omega \right)\right] + V(g_\omega h) \\
    & \geq V(g_\omega h),
\end{align*}
which completes the proof for [A5']. The first inequality holds because of the property of minimization, and the second inequality holds because $\hat{u}\left( g(s),\omega\right) - \hat{u}\left( f(s),\omega\right) \leq 0$ and $\phi(\omega), \psi_\omega(s) \geq 0$.

Next we prove that $\pfs$ satisfies [SC]. For any $f,g \in \F$ and $h,h' \in {\cal H}$, if $\omix{f}{h} \pfs \omix{g}{h}$, we have $V(\omix{f}{h}) \geq V(\omix{g}{h})$, we also have:
\[
V(\omix{g}{h}) \geq \min_{\phi \in \Phi} \phi(\omega) \left[ \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(g(s),\omega\right)  -  \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(f(s),\omega\right)\right] + V(\omix{f}{h}),
\]
which implies that $ \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(f(s),\omega\right)  - \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(g(s),\omega\right) \geq 0$. Thus we have:
\begin{align*}
V(\omix{f}{h'}) & \geq \min_{\phi \in \Phi} \phi(\omega) \left[ \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(f(s),\omega\right)  - \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(g(s),\omega\right) \right]  + V(\omix{g}{h'}) \\
& \geq V(\omix{g}{h'}),
\end{align*}
which shows that $\pfs$ satisfies [SC].

Then we prove that $\pfs$ satisfies [SWUA]. For any $f,g \in \F$ and $h \in {\cal H}$, $\alpha \in [0,1]$ if $f \sim_\omega g$, we have $V(\omix{f}{h}) = V(\omix{g}{h})$. Then we have that:
\begin{align*}
V(\omix{g}{h}) & \geq \min_{\phi \in \Phi} \phi(\omega) \left[  \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(g(s),\omega\right) - \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(f(s),\omega\right) \right] + V(\omix{f}{h}),
\end{align*}
which implies that $$\min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(g(s),\omega\right) - \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(f(s),\omega\right) \leq 0.$$ Then we have:
\begin{align*}
V(\omix{\convmix{f}{g}}{h}) & \geq (1-\alpha)\min_{\phi \in \Phi} \phi(\omega) \left[  \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(f(s),\omega\right) - \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(g(s),\omega\right) \right] + V(\omix{f}{h}) \\
& \geq V(\omix{g}{h}),  
\end{align*}
which shows that $\pfs$ satisfies [SWUA].

Then we prove that $\pfs$ satisfies [SC-I]. For any $f,g \in \F$, $h \in {\cal H}$, $\alpha \in (0,1)$ and $p \in \Delta(\mcx)$, if $\omix{f}{h} \pfs \omix{g}{h}$, similarly we have:
$$\min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(g(s),\omega\right) - \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(f(s),\omega\right) \leq 0.$$ Then we have:
Then we have:
\begin{align*}
V(\omix{(\convmix{f}{p})}{h}) & \geq \alpha \min_{\phi \in \Phi} \phi(\omega) \left[  \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(f(s),\omega\right) - \min_{\psi_\omega \in \Psi_\omega} \sum_s \psi_\omega(s) \hat{u} \left(g(s),\omega\right) \right]\\
& + V(\omix{(\convmix{g}{p})}{h}) \\
& \geq V(\omix{(\convmix{g}{p})}{h}),
\end{align*}
which shows that $\pfs$ satisfies [SC-I].

Then we prove $\pfs$ satisfies [SUA]. Since $V$ is $\olh,\ulh$-precise, for any $h,h' \in {\cal H}$, and $\alpha,\beta,\gamma \in [0,1]$, if we have 
$h\pfs \convexmix{\olh}{\ulh}{\beta}$ and $h' \pfs \convexmix{\olh}{\ulh}{\gamma}$, we have:
\begin{align*}
V(\convexmix{h}{h'}{\alpha}) & \geq \alpha V(h) + (1-\alpha)V(h') \\
& \geq \alpha V(\convexmix{\olh}{\ulh}{\beta}) + (1-\alpha)V(\convexmix{\olh}{\ulh}{\gamma}) \\
& = V\left(\convexmix{(\convexmix{\olh}{\ulh}{\beta})}{\convexmix{\olh}{\ulh}{\gamma}}{\alpha}\right),
\end{align*}
which proves that $\pfs$ satisfies [SUA]. The first inequality holds because the concavity of $V$, the first inequality holds because of the definition of $\pfs$, and 
the second inequality holds because $V$ is $\olh,\ulh-$precise.

Finaly, we prove that $\pfs$ satisfies [EC-I]. If $h \pfs h'$, for any $\alpha \in [0,1]$ and $\beta \in (0,1)$, similarly we have $h' \sim^* \convexmix{\olh}{\ulh}{\gamma}$ with a unique $\gamma \in [0,1]$, which is the result we proved in necessarity part,
we have:
\begin{align*}  
V\left(\convexmix{h}{\left(\convexmix{\olh}{\ulh}{\alpha}\right)}{\beta}\right) & \geq V(\beta h) +  V\left((1-\beta) \left(\convexmix{\olh}{\ulh}{\alpha}\right)\right) \\
& \geq V(\beta h') +  V\left((1-\beta) \left(\convexmix{\olh}{\ulh}{\alpha}\right)\right) \\
& = V\left(\beta \left(\convexmix{\olh}{\ulh}{\gamma}\right)\right) + V\left((1-\beta) \left(\convexmix{\olh}{\ulh}{\alpha}\right)\right)\\
& = V \left(\beta \left(\convexmix{\olh}{\ulh}{\gamma}\right) + (1-\beta) \left(\convexmix{\olh}{\ulh}{\alpha}\right)  \right) \\
& \geq V\left(\convexmix{h'}{\left(\convexmix{\olh}{\ulh}{\alpha}\right)}{\beta}\right).
\end{align*}

The first inequality holds because of the concavity of $V$, and the second inequality holds because $h \pfs h'$. The first and the second equality holds because 
$h' \sim \convexmix{\olh}{\ulh}{\gamma}$ and $V$ is $\olh,\ulh-$precise. The last inequality holds because $\pfs$ is [SUA].

Then if we have that $\convexmix{h}{\left(\convexmix{\olh}{\ulh}{\alpha}\right)}{\beta} \pfs \convexmix{h'}{\left(\convexmix{\olh}{\ulh}{\alpha}\right)}{\beta}$ we try to prove $h \pfs h'$.
We have that:
\begin{align*}
    & V \left( \convexmix{h}{\left(\convexmix{\olh}{\ulh}{\alpha}\right)}{\beta} \right) \geq V \left( \convexmix{h'}{\left(\convexmix{\olh}{\ulh}{\alpha}\right)}{\beta} \right) \\
    \implies & V(\beta h) + V\left((1-\beta)\left(\convexmix{\olh}{\ulh}{\alpha}\right)\right) \geq V(\beta h') + V\left((1-\beta)\left(\convexmix{\olh}{\ulh}{\alpha}\right)\right) \\ 
    \implies & V(\beta h) \geq V(\beta h')\\
    \implies & V(h) = \lim_{\beta \to 1^-} V(\beta h) \geq \lim_{\beta \to 1^-} V(\beta h') = V(h'),
\end{align*}
which shows that $h \pfs h'$. The first implication holds because of $V$ is $\olh,\ulh-$precise, and the third implication holds because $V$ is continuous in ${\cal H}$.
The above result shows that $\pfs$ satisfies [EC-I].

Hence we can conclude that (ii) $\implies$ (i).


\subsection{Proof of Theorem \ref{thm:DPRO}}
Based on the proof of Theorem \ref{thm:mental_DPRO}, we know that preference relationship has the form of Eq.~\eqref{eq:preference_star_representation}, and for each $\pfo$, it has representation function given by
Eq.\eqref{eq:preference_omega_representation}. Here we try to prove that (ii) in Theorem \ref{thm:mental_DPRO} is equivalent to (ii) in Theorem \ref{thm:DPRO}.

\subsubsection{Theorem \ref{thm:DPRO} (ii) $\Rightarrow$ Theorem \ref{thm:mental_DPRO} (ii)}

First we establish that each $\vartheta \in \mathcal{C}$ in Eq.~\eqref{eq:preference_representation} can be factorized into $\vartheta(\omega,s) = \phi(\omega)\psi(s|\omega)$ such that: (i) $\phi(\omega) \in \Phi$; and (ii) $\psi(s|\omega) \in \Psi_\omega$ for each $\omega$.
We begin our proof for this claim. Let us denote the representations in Eq.~\eqref{eq:preference_representation} as $V_{\text{joint}}(f)$ and Eq.~\eqref{eq:preference_star_representation} as $V_{\text{marg}}(f)$.

First we prove $V_{\text{marg}}(f) \leq V_{\text{joint}}(f)$.
We can factor $\vartheta(\omega,s)$ into $\phi(\omega)$ and $\psi(s|\omega)$. Then, we can rewrite the objective of Eq.~\eqref{eq:preference_representation} as:
$$
\sum_\omega \phi(\omega) \sum_s \psi(s|\omega)\sum_x u(x,\omega)f(x,s).
$$
Let $L(\omega,\psi)\triangleq\sum_s \psi(s|\omega)\sum_x u(x,\omega)f(x,s)$, then the objective can be equivalently written as $\sum_\omega \phi(\omega)L(\omega,\psi)$.

So for Eq.~\eqref{eq:preference_star_representation}, we have $\min_{\psi \in \Psi_\omega}L(\omega,\psi) \leq L(\omega,\psi)$. Hence, $\phi(\omega)\min_{\psi \in \Psi_\omega}L(\omega,\psi) \leq \phi(\omega ) L(\omega,\psi)$, since $\phi(\omega) \geq 0$, and we have 
\[
V_{\text{marg}}(f) = \min_{\phi \in \Phi} \sum_{\omega}\phi(\omega)\min_{\psi \in \Psi_\omega}L(\omega,\psi) \leq \sum_{\omega}\phi(\omega)L(\omega,\psi).
\]
Since the right hand side is arbitrary, we have $V_{\text{marg}}(f) \leq V_{\text{joint}}(f)$.

Next, we prove that $V_{\text{marg}}(f) \geq V_{\text{joint}}(f).$
For each $\omega$, let $\sigma_\omega^* = \text{arg} \min_{\psi \in \Psi_{\omega}} \, \sum_s \psi(s) \sum_x u(x,\omega)f(x,s)$. Then the objective of Eq.~\eqref{eq:preference_star_representation} is $$\sum_\omega \phi(\omega)\sum_s \sigma_\omega^*(s)\sum_x u(x,\omega)f(x,s).$$
We construct a set $\hat{\mathcal{C}} = \{ \vartheta(\omega,s):\vartheta(\omega,s) = \phi(\omega)\sigma_\omega^*(s), \forall s,\omega \}$.
Clearly, we have $\hat{\mathcal{C}} \subset \mathcal{C}$ and so
$$
V_\text{joint}(f) \leq \min_{\vartheta \in \hat{\mathcal{C}}} \sum_{\omega}\sum_s \vartheta(\omega,s)\sum_x u(x,\omega)f(x,s) = V_\text{marg}(f).
$$
Combining these two inequalities proves that $V_{\text{marg}}(f) = V_{\text{joint}}(f)$.

\subsubsection{Theorem \ref{thm:mental_DPRO} (ii) $\Rightarrow$ Theorem \ref{thm:DPRO} (ii)}

In this section, we prove that Theorem \ref{thm:mental_DPRO} (ii) $\Rightarrow$ Theorem \ref{thm:DPRO} (ii). Since $\Phi$ and each $\Psi_\omega$ are convex set, we establish a new set
$$
{\cal C} = \bigg\{ \vartheta: \vartheta = \phi(\omega)\psi(s|\omega), \phi(\omega) \in \Phi, \psi(s|\omega) \in \Psi_\omega \text{ for all } \omega \in \Omega \bigg\}.
$$

Pick any $\vartheta_1,\vartheta_2 \in {\cal C}$, we know that $\vartheta_1 = \phi_1(\omega)\psi_1(s|\omega)$ and $\vartheta_2 = \phi_2(\omega)\psi_2(s|\omega)$.
Then, for any $\alpha \in [0,1]$, we define $\vartheta_\alpha = \convmix{\vartheta_1}{\vartheta_2}$. Here we expresess $\phi_\alpha = \alpha \phi_1 + (1-\alpha) \phi_2$. Since $\Phi$ is a
convex set, we know $\phi_\alpha \in \Phi.$ Next we define $\psi_\alpha(s|\omega) = \frac{\convmix{\phi_1(\omega)\psi(s|\omega)}{\phi_2(\omega)\psi(s|\omega)}}{\alpha\phi_1(\omega) + (1-\alpha)\phi_2(\omega)}$. Obviously since 
$\Psi_\omega$ is a convex set, we know $\psi_\alpha(\cdot|\omega) \in \Psi_\omega$ for each $\omega$. By definition of ${\cal C}$, we know $\vartheta_\alpha = \phi_\alpha \psi(\cdot,|\omega) \in {\cal C}$, which shows ${\cal C}$ is closed and convex.

Similarly, using the same statements in proof of Theorem \ref{thm:DPRO} (ii) $\Rightarrow$ Theorem \ref{thm:mental_DPRO} (ii), we know that $V_{\text{marg}}(f) = V_{\text{joint}}(f)$. 


Thus, we showed statements (i) and (ii) are equivalent and the proof is complete.
In other words, our decision model is given by:
\[
    V(f) = \min_{\vartheta \in \mathcal{C}} \bbe_\vartheta \left[ u\left(w,f(s)\right) \right].
\]

\subsection{Proof of Theorem \ref{thm:optimization}}

Suppose $u(\cdot, \omega)$ is increasing and concave for all $\omega \in \Omega$. Then, $z \rightarrow u(r(z, s), \omega)$ is concave for all $(s, \omega)$ and the expected utility $\sum_{(\omega,s) \in (\Omega, \mcs)} \vartheta(\omega,s) u(r(z, s), \omega)$ is also concave. Finally,
$$
z \rightarrow \min_{\vartheta \in {\cal C}} \sum_{(\omega,s) \in (\Omega, \mcs)} \vartheta(\omega,s) u(r(z, s), \omega)
$$
is concave as the minimum of concave functions.


\section{Continuous Material and Mental States}
\label{sec:continuous}

Let ${\cal P}(\mcs)$ be the set of all probability distributions on the measurable space $(\mcs , {\cal B}(\mcs))$. Given a prior distribution $\psi \in {\cal P}(\mcs)$, $(\mcs , {\cal B}(\mcs), \psi)$ is a probability space.
We interpret $\psi$ as a representation of the beliefs of the decision maker (DM) about the underlying state of the world.

We equip $\mcs$ with the $\sigma$-algebra $\cal B(\mcs)$.
We equip $\Omega$ with the $\sigma-$algebra ${\cal B}(\Omega)$.

We let ${\cal B}(\mcs \times \Omega)$ be the $\sigma-$algebra on $\mcs \times \Omega$ and ${\cal P}(\mcs \times \Omega)$ be the set of all probability distributions on the measurable space $(\mcs \times \Omega, {\cal B}(\mcs \times \Omega))$.






\section{Additional Material for Section~\ref{sec:application} (Application)}

\subsection{Proof of Theorem \ref{thm:convex_formulation}}
Here we recall our original problem is:
\begin{align*}
    \max_{\boldsymbol{z}} \min_{\boldsymbol{\vartheta},\boldsymbol{\delta}} & \sum_{\omega,s}\vartheta(\omega,s) u(\boldsymbol{z},\omega,s) \\
    \text{s.t.}  \quad & \underline{c}_{m,\omega} \leq \sum_{s} \frac{\vartheta(\omega,s)}{\sum_{s'\in \mcs}\vartheta(\omega,s)} u(c_m,\omega,s) \quad \forall m \in [M], \omega \in \Omega, \\
    & \sum_{s,\omega} \vartheta(\omega,s) = 1, \\
    & \sum_{\omega,s} \delta_{\omega,s} \leq \epsilon, \\
    & \sum_{i=1} ^{N_\mathcal{Z}} z_i =1 \\
    & \begin{bmatrix}
        -\hat{\vartheta}(\omega,s) \\
        -\vartheta(\omega,s) \\
        \delta_{\omega,s}
    \end{bmatrix} \preceq_{K_{\text{exp}}} \begin{bmatrix}
        0 \\
        0 \\
        0
    \end{bmatrix} \quad \forall \omega \in \Omega, s\in \mcs,\\
    & \boldsymbol{z} \in \R^{N_\mathcal{Z}}_+, \boldsymbol{\vartheta} \in \R_+^{|\Omega|\times |\mcs|}, \boldsymbol{\delta}\in \Re^{|\Omega|\times |\mcs|}.
\end{align*}

Since the first constraint can be written as: $$\sum_s\vartheta(\omega,s) (\underline{c}_{m,\omega} - u(c_m,\omega,s)) \leq 0$$
We know that the whole original problem is a convex optimization problem. 

Then we consider the Lagarangian of the inner minmization problem, we let $\boldsymbol{\alpha}$, $\beta$, $\gamma$ and $\boldsymbol{\lambda}$ be the lagrange multipliers for the constraints. Then the Lagrangian is:
\begin{align*}
    \mathcal{L}(\boldsymbol{z},\boldsymbol{\vartheta},\boldsymbol{\delta},\boldsymbol{\alpha},\beta,\gamma,\boldsymbol{\lambda}) = & \sum_{\omega,s}\vartheta(\omega,s) u(\boldsymbol{z},\omega,s) + \sum_{m,\omega} \alpha_{m,\omega} \sum_s \vartheta(\omega,s) \left( \underline{c}_{m,\omega} - u(c_m,\omega,s)\right)\\
    & - \beta \left(1 - \sum_{\omega,s} \vartheta(\omega,s)\right) - \gamma \left( \epsilon - \sum_{\omega,s} \delta_{\omega,s}\right)\\
    & - \sum_{\omega,s} \lambda_{1}(\omega,s) \hat{\vartheta}(\omega,s) - \sum_{\omega,s} \lambda_{2}(\omega,s) \vartheta(\omega,s) - \sum_{\omega,s} \lambda_{3}(\omega,s) \delta_{\omega,s}.
\end{align*}

Based on this formulation and KKT conditions, we can write the dual of inner problem is:
\begin{align*}
\max_{\boldsymbol{\alpha},\beta,\gamma, \boldsymbol{\lambda}} & -\beta - \epsilon \gamma - \sum_{\omega,s} \hat{\vartheta}(\omega,s)\lambda_1(\omega,s)\\
\text{s.t.} \quad &u(\boldsymbol{z},\omega,s) - \sum_m \alpha_{m,\omega} \left( \underline{c}_{\omega,m} - u(c_m,\omega,s)\right) + \beta - \lambda_2(\omega,s) \leq 0 \quad \forall \omega,s\\
& \gamma + \lambda_3(\omega,s) = 0 \quad \forall \omega,s\\
& \boldsymbol{\alpha} \in \R_+^{M\times |\Omega|}, \beta , \gamma \in \Re, (\lambda_1(\omega,s), \lambda_2(\omega,s),\lambda_3(\omega,s))\in \left(K_{\text{exp}}\right)_*.
\end{align*}

Thus, our initial problem can be written as the following convex linear conic programming:
\begin{align*}
    \max_{\boldsymbol{z},\boldsymbol{\alpha},\beta,\gamma, \boldsymbol{\lambda}} & -\beta - \epsilon \gamma - \sum_{\omega,s} \hat{\vartheta}(\omega,s)\lambda_1(\omega,s)\\
\text{s.t.} \quad &u(\boldsymbol{z},\omega,s) - \sum_m \alpha_{m,\omega} \left( \underline{c}_{\omega,m} - u(c_m,\omega,s)\right) + \beta - \lambda_2(\omega,s) \leq 0 \quad \forall \omega,s\\
& \gamma + \lambda_3(\omega,s) = 0 \quad \forall \omega,s\\
&\sum_{i=1}^{N_\mathcal{Z}} z_i = 1\\
& \boldsymbol{z}\in \R^{N_\mathcal{Z}}_+,\boldsymbol{\alpha} \in \R_+^{M\times |\Omega|}, \beta , \gamma \in \Re, (\lambda_1(\omega,s), \lambda_2(\omega,s),\lambda_3(\omega,s))\in \left(K_{\text{exp}}\right)_*.
\end{align*}

And above formulation is a linear conic programming problem, which concludes our proof.

\section{Additional Material for Section~\ref{sec:discussion} (Discussion)}
\label{sec:appendix_discussion}


\subsection{Subjective Expected Utility}

Here we connect our model to the classical state-dependent SEU model, without any ambiguity.
The strongest version of the independence axiom is:
%
\begin{itemize}
\item[I] (Independence) Let $f, g, h \in {\cal F}$, then $f \succeq g$ if and only if $\alpha f + (1 - \alpha) h \succeq \alpha g + (1 - \alpha) h$ for all $\alpha \in [0, 1]$.
\end{itemize}
Axiom [I] means that the preference $f \succeq g$ is indifferent to mixing with any other act.

Next we recall the well-known basic representation theorem.

%
\begin{thm}
\label{thm:BRT}(Basic Representation Theorem (BRT))
The following statements are equivalent:

(i) $\succeq$ is an binary relation on a convex, nonempty subset ${\cal C}$ of an Euclidean space and satisfies Axioms [A1] - [A5] and [I].

(ii) There exist an affine function $V : {\cal C} \rightarrow \R$ such that for all $a,b \in {\cal C}$, $a \succeq b$ if and only if $V(a) \geq V(b)$.
\end{thm}

Under the strongest independence axiom [I], we have the following well-known representation result for subjective expected utility (SEU).
This model is ambiguity neutral, we use it throughout as a benchmark to characterize ambiguity averse attitudes.
%
\begin{thm}
\label{thm:SEU}
(Subjective Expected Utility (SEU))
The following statements are equivalent:

(i) $\succeq$ satisfies Axioms [A1] - [A5] and [I].

(ii) There exists $u \in \mathscr{U}({\cal X})$ and $\vartheta \in \calps$ such that $f \succeq g$ if and only if $V(f) \geq V(g)$ where
%
\begin{equation}
\label{eq:SEU}
V(f) = \bbe_{\vartheta}[u(f)].
\end{equation}
%
\end{thm}
\noindent
In Theorem~\ref{thm:SEU}, beliefs and tastes are both fully specified by DM's preference relation (i.e., there is no ambiguity in either beliefs or tastes).

\cite{karni2016expected} developed a decision model based on SEU to characterize a state-dependent preference DM's value function. 
%
\begin{itemize}
\item[C] (Consistency) For all $s\in\mcs$ and all semipositive $\ell,\ell'\in L$, such that $\ell(s') \sim \ell'(s')$ for all $s'\neq s$: if $H(\ell) \succeq H(\ell')$ then $\ell \succeq^* \ell'$.
\end{itemize}
%
%
\begin{thm}
\label{thm:SSEU}
(State-Dependent Subjective Expected Utility (SSEU))
The following statements are equivalent:

(i) $\succeq$ and $\succeq^*$ satisfy Axioms [A1] - [A5] and [I] and jointly satisfy [C].

(ii) There exists $u \in \mathscr{U}({\cal X})$ and $\vartheta \in \calps$ such that $f \succeq g$ if and only if $V(f) \geq V(g)$ where
%
\begin{equation}
\label{eq:SSEU_act}
V(f) = \bbe_{\vartheta}[u(f,s)],
\end{equation}
%
and for all $\ell,\ell' \in L$, $\ell \succeq^* \ell'$ if and only if $V^*(\ell) \geq V^*(\ell')$ where
%
\begin{equation}
\label{eq:SSEU_lottery}
V^*(\ell) = \sum_{s\in \mcs}\sum_{x\in {\cal X}(s)} u(x,s)\ell(x,s).
\end{equation}
%
\end{thm}
\noindent
Theorem~\ref{thm:SSEU} extends Theorem~\ref{thm:SEU} by allowing DM to have a different utility function (and different tastes) in every state.


\subsection{Proof of Thorem \ref{thm:tastes_only}}

Necessity ((ii) $\Rightarrow$ (i)) is trivial, so we just show sufficiency ((i) $\Rightarrow$ (ii)).
We continue to use the definition for state-dependent preferences (Definition \ref{defn:state_dep_preference}) from the proof of Theorem~\ref{thm:DPRO}.

Since $\pfs$ satisfies [A5'], by Lemma \ref{lem:state_dep_preference_axiom_lem}, we know that every $\pfo$ satisfies [A5] (Monotonicity). And we proceed with the following lemma. Recall [I] is the strongest independence axiom in Appendix~\ref{sec:appendix_axioms}.

\begin{lem}
\label{lem:state_dep_preference_independence_lem}
Suppose $\pfs$ satisfies [SI], then $\pfo$ satisfies [I] for all $\omega \in \Omega$.  
\end{lem}
\begin{proof}[Proof of Lemma \ref{lem:state_dep_preference_independence_lem}]
For every $f,f',f'' \in \F$, if $f\pfo f'$, then by definition of $\pfo$ there exist $h \in {\cal H}$ such that $\omix{f}{h} \pfs \omix{f'}{h}$. Then, axiom
[SI] guarantees that $  \omix{\left(\convmix{f}{f''}\right)}{h} \pfs \omix{\left(\convmix{f'}{f''}\right)}{h}$ for every $\alpha \in (0,1)$. Again by definition of
$\pfo$, we have that $f\pfo f'$ implies $\convmix{f}{f''} \pfo \convmix{f'}{f''}$, which completes the proof.   
\end{proof}

Hence, $\pfo$ is a continuous weak order on $\F$ which satisfies [A1] - [A5] and [I]. By SEU Theorem \ref{thm:SEU}, for each $\omega$ there exists a subjective prior $\psi \in \calps$ and an evaluation function $v(\cdot,\omega): \F \to \R$ of the form:
\[
v(f,\omega) = \sum_{s\in \mcs} \psi(s)\sum_{x \in \mcx} u(x,\omega) f(x,s).
\]

As in the proof of Theorem~\ref{thm:DPRO} and by Proposition \ref{prop:aggregation_proposition}, we conclude that there exists such a closed convex set ${\cal C} \in \mathcal{P}(\Omega)$ and a subjective prior $\psi \in \calps$ such that for every $f \in \F$, we have:
\[
V(f) = \min_{\vartheta \in {\cal C}} \sum_{\omega \in \Omega} \vartheta(\omega) \sum_{s\in \mcs} \psi(s) \sum_{x\in \mcx} u(x,\omega)f(x,s).
\]

So here, we know that if we evaluate constant AA act $p$, we have:
\begin{align*}
    V(p) &= \min_{\vartheta \in {\cal C}} \sum_{\omega \in \Omega} \vartheta(\omega) \sum_{s\in \mcs} \psi(s) \sum_{x\in \mcx} u(x,\omega)p(x) \\
    &= \min_{\vartheta \in {\cal C}} \sum_{\omega \in \Omega} \vartheta(\omega) \sum_{x\in \mathcal{X}}u(x,\omega)p(x),
\end{align*}

where the second equality holds because $p$ is constant and $\sum_{s\in \mcs} \psi(s) = 1$. 

\subsection{Proof of Theorem \ref{thm:MEU_Pessimistic_MEU}}
\subsubsection{Sufficiency: (i) $\Rightarrow$ (ii)}

We recall the definition for $\pfo$ defined in \ref{defn:state_dep_preference}. Given the proof of Lemma \ref{lem:state_dep_preference_lem} we know 
$\pfo$ is a continus weak order for every $\omega$.

Then we give the proof that if $\pfo$ satisfies [SC*], then $\pfo$ satisfies:
%
\begin{itemize}
    \item [SC for $\pfo$] (State Consistency for $\pfo$): For every $f,f' \in F$, $p,q \in \calpx$ and $s\in \mcs$, if $\smix{p}{f} \pfo \smix{q}{f}$  we have
    $\smix{p}{f'} \pfo \smix{q}{f'}$.
\end{itemize}
%
\begin{proof}
    Given [SC*], we know for each $\omega \in \Omega$, $h,h'\in {\cal H}$, $f,g \in \F$, $p,q \in \calpx$ and every $s\in \mcs$, if $\omix{(p_s f)}{h} \pfs \omix{(q_s f)}{h}$, we have
    $\omix{(p_s g)}{h'} \pfs \omix{(q_s g)}{h'}$. Then given the definition of $\pfo$, we could know that [SC for $\pfo$] holds.
\end{proof}

Then given the Theorem 1 in \cite{hill2019non}, we know that if $\pfo$ is a continus weak order and satisfies [SC for $\pfo$], [SUA*] and [EC-I*], there exists a representation function 
for each $\pfo$ which takes the form:
\[
v(\omega,f) = \min_{\psi \in \calps} \sum_{s\in \mcs} \psi(s) \min_{u \in v(\omega,s)} \sum_{x\in \mcx} u\big(f(s)\big).
\]

Then following standard procedure which we did in proof of Theorem \ref{thm:mental_DPRO} and Theorem \ref{thm:DPRO} and because of Proposition \ref{prop:aggregation_proposition}, we can conclude that the representation function for
$\pfo$ takes the form of:
\[
V(h) = \min_{\vartheta \in \mathcal{C}} \sum_{\omega \in \Omega}  \sum_{s\in \mcs} \vartheta(\omega,s) \min_{u \in v(\omega,s)} u\big(f(s)\big).
\]

\end{document}